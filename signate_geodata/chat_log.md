# Signate ç‰©ä»¶ä¾¡æ ¼äºˆæ¸¬
_Exported on 2026/2/19 at 6:14:20 JST from Cursor (2.4.31)_

---

**User**

SIGNATEã§ä»¥ä¸‹ã®äºˆæ¸¬ã«å–ã‚Šçµ„ã¿ã¾ã™ï¼äºˆæ¸¬ç”¨ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ä¸‹ã•ã„ï¼å‡ºåŠ›å½¢å¼ã¯sample_submit.csvã‚’å‚è€ƒã«ã—ã¦ä¸‹ã•ã„ï¼
## å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ï¼štrain.csvã«ã¯ã€2019å¹´ã€œ2022å¹´ã®1â½‰ã¾ãŸã¯7â½‰ã«LIFULLãƒãƒ¼ã‚¿ãƒ«ä¸Šã«æ²è¼‰ã•ã‚ŒãŸç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€éå»ã®å£²è²·ä¾¡æ ¼(money_room)ã‚„ç‰©ä»¶ã®ç‰¹å¾´ã‚’å«ã‚“ã§ãŠã‚Šã€å£²è²·ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã«ä½¿â½¤ã§ãã¾ã™ã€‚
## è©•ä¾¡â½¤ãƒ‡ãƒ¼ã‚¿ï¼štest.csvã«ã¯ã€å£²è²·ä¾¡æ ¼(money_room)ã‚’é™¤ã2023å¹´ã®1â½‰ã¾ãŸã¯7â½‰ã«LIFULLãƒãƒ¼ã‚¿ãƒ«ä¸Šã«æ²è¼‰ã•ã‚ŒãŸç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚idã¨ã„ã†ã‚µãƒ³ãƒ—ãƒ«IDãŒæ–°ãŸã«ä»˜ä¸ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«å£²è²·ä¾¡æ ¼ï¼ˆmoney_roomï¼‰ã‚’äºˆæ¸¬ã—ã¦ã„ãŸã ãã¾ã™ã€‚
## è©•ä¾¡æ–¹æ³•ï¼šè©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ã€ŒMAPEï¼ˆMean Absolute Percentage Errorï¼‰ã€ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚MAPEã¯æ­£è§£å€¤ã®çµ¶å¯¾å€¤ã¨å®Ÿéš›ã®å€¤ã¨ã®çµ¶å¯¾èª¤å·®ã®æ¯”ã®ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã®å¹³å‡å€¤ã‚’å–ã£ãŸã‚‚ã®ã§ã€äºˆæ¸¬ç²¾åº¦ãŒé«˜ã„ã»ã©å€¤ãŒå°ã•ããªã‚Šã¾ã™ã€‚äºˆæ¸¬å€¤ã¯ã‚ã‚‹å¹´æœˆã«ãŠã‘ã‚‹ã‚ã‚‹ç‰©ä»¶ã®å£²è²·ä¾¡æ ¼(å˜ä½ã¯å††)ã¨ã—ã¾ã™ã€‚

---

**Cursor**

ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ã‚’ç¢ºèªã•ã›ã¦ãã ã•ã„ã€‚

ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ ãŒåˆ†ã‹ã‚Šã¾ã—ãŸã€‚æ¬¡ã«ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã¨ç‰¹å¾´é‡ã‚’ã‚‚ã†å°‘ã—ç¢ºèªã—ã¾ã™ã€‚

ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã‚’æŠŠæ¡ã—ã¾ã—ãŸï¼š
- train.csv: ç´„36ä¸‡è¡Œã€149ã‚«ãƒ©ãƒ 
- test.csv: ç´„11ä¸‡è¡Œ
- æå‡ºå½¢å¼: id, money_room (äºˆæ¸¬ä¾¡æ ¼)

ãã‚Œã§ã¯ã€ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã™ã€‚

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - SIGNATE åœ°ç†ç©ºé–“ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³
è©•ä¾¡æŒ‡æ¨™: MAPE (Mean Absolute Percentage Error)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test

def mape(y_true, y_pred):
    """MAPEè¨ˆç®—"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def mape_lgb(y_pred, data):
    """LightGBMç”¨MAPEãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    y_true = data.get_label()
    score = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return 'mape', score, False

def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = 'money_room'
    y_train = train[target].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = [target, 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ ï¼ˆå¤šãã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã‚’æŒã¤å¯èƒ½æ€§ï¼‰
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    # é‡è¤‡å‰Šé™¤
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
    for df in [train, test]:
        # å°‚æœ‰é¢ç©ã‚ãŸã‚Šã®ç‰¹å¾´é‡è¨ˆç®—ç”¨
        if 'unit_area' in df.columns and 'total_floor_area' in df.columns:
            df['unit_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
    
    for col in ['unit_area_ratio', 'land_coverage', 'house_land_ratio']:
        if col in train.columns and col not in num_cols:
            num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    print(f"  - æ•°å€¤ç‰¹å¾´é‡: {len([c for c in use_cols if c in num_cols])}")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len([c for c in use_cols if c in cat_cols])}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    label_encoders = {}
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            # trainã¨testã‚’çµåˆã—ã¦fit
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            label_encoders[col] = le
    
    # æ¬ æå€¤å‡¦ç†ï¼ˆæ•°å€¤ã¯-999ã§åŸ‹ã‚ã‚‹ï¼‰
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols

def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = {
        'objective': 'regression',
        'metric': 'mape',
        'boosting_type': 'gbdt',
        'learning_rate': 0.05,
        'num_leaves': 127,
        'max_depth': -1,
        'min_child_samples': 20,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    n_splits = 5
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    oof_pred = np.zeros(len(X_train))
    models = []
    
    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
        print(f"\nFold {fold + 1}/{n_splits}")
        
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
        y_tr, y_val = y_train[train_idx], y_train[valid_idx]
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
        
        train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
        valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=2000,
            valid_sets=[train_data, valid_data],
            valid_names=['train', 'valid'],
            feval=mape_lgb,
            callbacks=[
                lgb.early_stopping(stopping_rounds=100, verbose=True),
                lgb.log_evaluation(period=200)
            ]
        )
        
        models.append(model)
        oof_pred[valid_idx] = model.predict(X_val)
        
        fold_mape = mape(y_val, oof_pred[valid_idx])
        print(f"Fold {fold + 1} MAPE: {fold_mape:.4f}%")
    
    overall_mape = mape(y_train, oof_pred)
    print(f"\n=== Overall OOF MAPE: {overall_mape:.4f}% ===")
    
    return models

def predict_and_submit(models, X_test, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§äºˆæ¸¬
    predictions = np.zeros(len(X_test))
    for model in models:
        predictions += model.predict(X_test)
    predictions /= len(models)
    
    # è² ã®å€¤ã‚’è£œæ­£ï¼ˆä¾¡æ ¼ã¯æ­£ã®å€¤ã®ã¿ï¼‰
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    
    # idã§ã‚½ãƒ¼ãƒˆ
    submission = submission.sort_values('id')
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    output_path = f'{OUTPUT_DIR}/submission.csv'
    submission.to_csv(output_path, index=False, header=False)
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"äºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    
    return submission

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_ids)
    
    print("\n=== å‡¦ç†å®Œäº† ===")
    
    return submission

if __name__ == '__main__':
    main()

```

ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã¯æˆåŠŸã—ã¾ã—ãŸï¼ˆOOF MAPE: 15.79%ï¼‰ã€‚äºˆæ¸¬æ™‚ã«ãƒ‡ãƒ¼ã‚¿å‹ã®ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹ã®ã§ä¿®æ­£ã—ã¾ã™ã€‚

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - SIGNATE åœ°ç†ç©ºé–“ãƒ‡ãƒ¼ã‚¿æ´»ç”¨ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³
è©•ä¾¡æŒ‡æ¨™: MAPE (Mean Absolute Percentage Error)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test

def mape(y_true, y_pred):
    """MAPEè¨ˆç®—"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

def mape_lgb(y_pred, data):
    """LightGBMç”¨MAPEãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    y_true = data.get_label()
    score = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return 'mape', score, False

def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = 'money_room'
    y_train = train[target].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = [target, 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ ï¼ˆå¤šãã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤ã‚’æŒã¤å¯èƒ½æ€§ï¼‰
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    # é‡è¤‡å‰Šé™¤
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
    for df in [train, test]:
        # å°‚æœ‰é¢ç©ã‚ãŸã‚Šã®ç‰¹å¾´é‡è¨ˆç®—ç”¨
        if 'unit_area' in df.columns and 'total_floor_area' in df.columns:
            df['unit_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
    
    for col in ['unit_area_ratio', 'land_coverage', 'house_land_ratio']:
        if col in train.columns and col not in num_cols:
            num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    print(f"  - æ•°å€¤ç‰¹å¾´é‡: {len([c for c in use_cols if c in num_cols])}")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len([c for c in use_cols if c in cat_cols])}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    label_encoders = {}
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            # trainã¨testã‚’çµåˆã—ã¦fit
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            label_encoders[col] = le
    
    # æ¬ æå€¤å‡¦ç†ï¼ˆæ•°å€¤ã¯-999ã§åŸ‹ã‚ã‚‹ï¼‰
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols

def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = {
        'objective': 'regression',
        'metric': 'mape',
        'boosting_type': 'gbdt',
        'learning_rate': 0.05,
        'num_leaves': 127,
        'max_depth': -1,
        'min_child_samples': 20,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }
    
    # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    n_splits = 5
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    oof_pred = np.zeros(len(X_train))
    models = []
    
    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
        print(f"\nFold {fold + 1}/{n_splits}")
        
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
        y_tr, y_val = y_train[train_idx], y_train[valid_idx]
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚«ãƒ©ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
        cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
        
        train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
        valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=2000,
            valid_sets=[train_data, valid_data],
            valid_names=['train', 'valid'],
            feval=mape_lgb,
            callbacks=[
                lgb.early_stopping(stopping_rounds=100, verbose=True),
                lgb.log_evaluation(period=200)
            ]
        )
        
        models.append(model)
        oof_pred[valid_idx] = model.predict(X_val)
        
        fold_mape = mape(y_val, oof_pred[valid_idx])
        print(f"Fold {fold + 1} MAPE: {fold_mape:.4f}%")
    
    overall_mape = mape(y_train, oof_pred)
    print(f"\n=== Overall OOF MAPE: {overall_mape:.4f}% ===")
    
    return models

def predict_and_submit(models, X_test, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§äºˆæ¸¬
    predictions = np.zeros(len(X_test))
    for model in models:
        predictions += model.predict(X_test)
    predictions /= len(models)
    
    # è² ã®å€¤ã‚’è£œæ­£ï¼ˆä¾¡æ ¼ã¯æ­£ã®å€¤ã®ã¿ï¼‰
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    
    # idã§ã‚½ãƒ¼ãƒˆ
    submission = submission.sort_values('id')
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    output_path = f'{OUTPUT_DIR}/submission.csv'
    submission.to_csv(output_path, index=False, header=False)
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"äºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    
    return submission

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_ids)
    
    print("\n=== å‡¦ç†å®Œäº† ===")
    
    return submission

if __name__ == '__main__':
    main()


```

---

**User**

ã“ã‚Œã¯ã©ã®ã‚ˆã†ã«å®Ÿè¡Œã™ã‚Œã°è‰¯ã„ã§ã™ã‹ï¼Ÿ

---

**Cursor**

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§å®Ÿè¡Œã§ãã¾ã™ï¼š

```bash
cd /home/ubuntu/signate/geospatial_data_challenge_2nd
python src/predict.py
```

å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ

```shellscript
cd /home/ubuntu/signate/geospatial_data_challenge_2nd
python src/predict.py
```

å®Ÿè¡Œå®Œäº†ã—ã¾ã—ãŸï¼çµæœã‚’ã”èª¬æ˜ã—ã¾ã™ï¼š

## å®Ÿè¡Œçµæœ

**ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ï¼ˆ5-Fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰:**
- Fold 1 MAPE: 15.73%
- Fold 2 MAPE: 15.72%
- Fold 3 MAPE: 15.83%
- Fold 4 MAPE: 15.72%
- Fold 5 MAPE: 15.81%
- **Overall OOF MAPE: 15.76%**

**äºˆæ¸¬çµ±è¨ˆ:**
- äºˆæ¸¬ä»¶æ•°: 112,437ä»¶
- å¹³å‡ä¾¡æ ¼: ç´„2,837ä¸‡å††
- ä¸­å¤®å€¤: ç´„2,428ä¸‡å††
- æœ€å°: ç´„165ä¸‡å††
- æœ€å¤§: ç´„1.7å„„å††

**æå‡ºãƒ•ã‚¡ã‚¤ãƒ«:**

```
/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit/submission.csv
```

æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š

```plaintext
/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit/submission.csv
```

```plaintext
/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit/submission.csv
```

æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚

## å®Ÿè¡Œæ–¹æ³•ã®ã¾ã¨ã‚

```bash
cd /home/ubuntu/signate/geospatial_data_challenge_2nd
python src/predict.py
```

## æå‡ºãƒ•ã‚¡ã‚¤ãƒ«
- **ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹:** `/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit/submission.csv`
- **å½¢å¼:** `id,money_room`ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ã€sample_submit.csvã¨åŒã˜å½¢å¼ï¼‰

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’SIGNATEã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚Œã°æå‡ºå®Œäº†ã§ã™ï¼

```shellscript
cd /home/ubuntu/signate/geospatial_data_challenge_2nd
python src/predict.py
```

```shellscript
cd /home/ubuntu/signate/geospatial_data_challenge_2nd
python src/predict.py
```

---

**User**

ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼æå‡ºã—ãŸã¨ã“ã‚ï¼Œã‚¹ã‚³ã‚¢ã¯17ãã‚‰ã„ã§ã—ãŸï¼

ã‚ãªãŸã¯Webãƒšãƒ¼ã‚¸ã®URLã‚’å…¥åŠ›ã«å…¥ã‚Œã‚Œã°ï¼Œãã®ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ç†è§£ã—å›ç­”ã—ã¦ãã‚Œã¾ã™ã‹ï¼Ÿ

å‰å¹´ã®ã‚³ãƒ³ãƒšã®å…¥è³è€…ã®è¨˜äº‹ã‚’è¦‹ã¤ã‘ãŸã®ã§ã™ãŒï¼ï¼ï¼

---

**Cursor**

ã‚¹ã‚³ã‚¢17%ã¨ã®ã“ã¨ã€ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼

## URLã«ã¤ã„ã¦

æ®‹å¿µãªãŒã‚‰ã€**URLã‚’ç›´æ¥å…¥åŠ›ã—ã¦ãƒšãƒ¼ã‚¸å†…å®¹ã‚’èª­ã¿è¾¼ã‚€æ©Ÿèƒ½ã¯æŒã£ã¦ã„ã¾ã›ã‚“**ã€‚ãŸã ã—ã€ä»¥ä¸‹ã®æ–¹æ³•ã§å¯¾å¿œã§ãã¾ã™ï¼š

1. **Webæ¤œç´¢æ©Ÿèƒ½**: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§é–¢é€£æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã“ã¨ã¯å¯èƒ½ã§ã™
2. **è¨˜äº‹å†…å®¹ã®ã‚³ãƒ”ãƒ¼**: è¨˜äº‹ã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã“ã®ãƒãƒ£ãƒƒãƒˆã«è²¼ã‚Šä»˜ã‘ã¦ã„ãŸã ã‘ã‚Œã°ã€ç†è§£ã—ã¦å›ç­”ã§ãã¾ã™

## å‰å¹´ã®ã‚³ãƒ³ãƒšã«ã¤ã„ã¦

å‰å¹´ã®ã‚³ãƒ³ãƒšã®å…¥è³è€…ã®è§£æ³•ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã‹ï¼Ÿ

ã‚‚ã†å°‘ã—å…·ä½“çš„ã«æ¤œç´¢ã—ã¦ã¿ã¾ã™ï¼š

æ¤œç´¢ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã­ã€‚

**ãŠå‹§ã‚ã®æ–¹æ³•ï¼š**

æ—¢ã«å…¥è³è€…ã®è¨˜äº‹ã‚’è¦‹ã¤ã‘ã¦ã„ã‚‹ã¨ã®ã“ã¨ã§ã™ã®ã§ã€ãã®è¨˜äº‹ã®**é‡è¦ãªéƒ¨åˆ†ï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«é¸æŠã€å‰å‡¦ç†ãªã©ï¼‰ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã“ã®ãƒãƒ£ãƒƒãƒˆã«è²¼ã‚Šä»˜ã‘ã¦**ã„ãŸã ã‘ã‚Œã°ã€ãã®å†…å®¹ã‚’ç†è§£ã—ã¦ã€ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰ã«å–ã‚Šå…¥ã‚Œã‚‹æ”¹å–„ç‚¹ã‚’ã”ææ¡ˆã§ãã¾ã™ã€‚

ç‰¹ã«çŸ¥ã‚ŠãŸã„æƒ…å ±ãŒã‚ã‚Œã°ï¼š
- ä½¿ç”¨ã—ãŸç‰¹å¾´é‡
- ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ–¹æ³•
- ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†æ‰‹æ³•
- å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨æ–¹æ³•

ãªã©ã‚’ãŠçŸ¥ã‚‰ã›ãã ã•ã„ï¼

---

**User**

è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒšã—ã¦è²¼ã‚Šä»˜ã‘ã¾ã™ï¼

å‚è€ƒã«ã§ããã†ãªã¨ã“ã‚ãŒã‚ã‚Œã°å‚è€ƒã«ã—ã¦ä¸‹ã•ã„ï¼



```

å›½äº¤çœã‚³ãƒ³ãƒšå„ªå‹ï¼è³ƒæ–™äºˆæ¸¬ã‚³ãƒ³ãƒš1ä½è§£æ³•

kenkoooo

technology



ã“ã‚“ã«ã¡ã¯ã€ã‚¹ã‚¿ãƒƒãƒ•ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã® @kenkoooo ã§ã™ï¼

å…ˆæ—¥é–‹å‚¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã‚³ãƒ³ãƒšã€Œç¬¬1å› å›½åœŸäº¤é€šçœ åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ ï½å›½åœŸæ•°å€¤æƒ…å ±ç·¨ï½ ãƒ¢ãƒ‡ãƒªãƒ³ã‚°éƒ¨é–€ã€ã§å„ªå‹ã—ã¾ã—ãŸï¼(æœ€çµ‚é †ä½è¡¨)



ã“ã®è¨˜äº‹ã§ã¯1ä½è§£æ³•ã‚’è§£èª¬ã—ã¦ã„ã¾ã™ã€‚ä¸å‹•ç”£ãƒ‡ãƒ¼ã‚¿åˆ†æã®é›°å›²æ°—ãŒä¼ã‚ã‚Œã°å¹¸ã„ã§ã™ã€‚



ç¬¬1å› å›½åœŸäº¤é€šçœ åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ ï½å›½åœŸæ•°å€¤æƒ…å ±ç·¨ï½ ãƒ¢ãƒ‡ãƒªãƒ³ã‚°éƒ¨é–€ã¨ã¯

å›½åœŸäº¤é€šçœãŒä¸»å‚¬ã™ã‚‹ã€è³ƒè²¸ä½å®…ã®è³ƒæ–™ã‚’äºˆæ¸¬ã™ã‚‹ã‚³ãƒ³ãƒšã§ã™ã€‚æ ªå¼ä¼šç¤¾LIFULLãŒæä¾›ã™ã‚‹å…¨å›½ã®è³ƒè²¸ãƒãƒ³ã‚·ãƒ§ãƒ³ãƒ»ã‚¢ãƒ‘ãƒ¼ãƒˆã®è³ƒæ–™ã¨ç‰©ä»¶æƒ…å ±ã‚’å…ƒã«ã€ä¸ãˆã‚‰ã‚ŒãŸè³ƒè²¸ç‰©ä»¶ã®è³ƒæ–™ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚



ä¸Šè¨˜ãƒ‡ãƒ¼ã‚¿ã®ä»–ã«ã€å›½åœŸäº¤é€šçœãŒæä¾›ã™ã‚‹ã€Œå›½åœŸæ•°å€¤æƒ…å ±ã€ã¨ã„ã†ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚ã¾ãŸã€æ ªå¼ä¼šç¤¾ã‚¼ãƒ³ãƒªãƒ³ãŒæä¾›ã™ã‚‹ ZENRIN Maps API ã‚„ã€SnowflakeåˆåŒä¼šç¤¾ãŒæä¾›ã™ã‚‹ Snowflake ç’°å¢ƒãªã©ã‚‚åˆ©ç”¨ã§ãã¾ã™ã€‚



1ä½è§£æ³•

LightGBM ã‚’ä½¿ã†

LightGBM ã‚’ç´ ã®ã¾ã¾ä½¿ã„ã¾ã—ãŸã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚è¡Œã„ã¾ã›ã‚“ã§ã—ãŸãŒã€seed ã‚’å¤‰ãˆãŸãƒ¢ãƒ‡ãƒ«ã‚’ã„ãã¤ã‹ç”¨æ„ã—ã€ãã‚Œã‚‰ã®äºˆæ¸¬å€¤ã®å¹³å‡ã‚’ã¨ã‚Šã¾ã—ãŸã€‚



PARAMS = {

    "objective": "regression",

    "metric": "l1",

    "seed": 42,

    "learning_rate": 0.1,

}



lgb_train = lgb.Dataset(X_train, y_train)

lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train)

model = lgb.train(

    PARAMS,

    lgb_train,

    valid_sets=lgb_eval,

    num_boost_round=100000,

    callbacks=[

        lgb.early_stopping(stopping_rounds=100),

        lgb.log_evaluation(100),

    ],

)

å¹³ç±³å˜ä¾¡ã®èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹

ã‚³ãƒ³ãƒšã®è©•ä¾¡æŒ‡æ¨™ã¯è³ƒæ–™ã®äºˆæ¸¬å€¤ã® RMSE ã¨ãªã£ã¦ã„ã‚‹ã®ã§ã€æœ€åˆã¯ã“ã‚Œã«å¾“ã„ã€è³ƒæ–™ã® RMSE ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã›ã¦ã„ã¾ã—ãŸã€‚ã—ã‹ã—ã€æ‰‹å…ƒã® Cross Validation ã‚’è¦‹ã‚‹ã¨ Fold ã”ã¨ã«å¤§ããã‚¹ã‚³ã‚¢ãŒç•°ãªã‚‹ã“ã¨ã«æ°—ã¥ãã¾ã—ãŸã€‚Fold å†…ã®äºˆæ¸¬å€¤ã‚’è¦‹ã¦ã¿ã‚‹ã¨ã€è³ƒæ–™ãŒ50ä¸‡å††ã‚’è¶…ãˆã‚‹ã‚ˆã†ãªå°‘æ•°ã®å¤–ã‚Œå€¤ã«æŒ¯ã‚Šå›ã•ã‚Œã¦ã„ã‚‹é›°å›²æ°—ã‚’æ„Ÿã˜ã¾ã—ãŸã€‚



estie ã§ã‚ˆãæ‰±ã†ã‚ªãƒ•ã‚£ã‚¹è³ƒè²¸ã®ç‰©ä»¶ã§ã¯ã€è³ƒæ–™ç·é¡ã‚ˆã‚Šã‚‚å˜ä½é¢ç©ã‚ãŸã‚Šã®è³ƒæ–™ï¼ˆã‚ªãƒ•ã‚£ã‚¹ã®å ´åˆã¯åªå˜ä¾¡ï¼‰ã«æ³¨ç›®ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚ã“ã‚Œã¯ã€å¥‘ç´„å‰ã‹ã‚‰é–“å–ã‚Šã‚„é¢ç©ãŒå›ºå®šã•ã‚Œã¦ã„ã‚‹ä½å®…ã¨ç•°ãªã‚Šã€ã‚ªãƒ•ã‚£ã‚¹ã¯ä½•ã‚‚ãªã„ãƒ•ãƒ­ã‚¢ã«å¥‘ç´„å¾Œã«å£ã‚’ç«‹ã¦ã¦å¥‘ç´„ã—ãŸåºƒã•ã®åŒºç”»ã‚’ä½œã‚‹ãŸã‚ã€å¥‘ç´„æ™‚ã¾ã§è³ƒæ–™ç·é¡ãŒæ±ºã¾ã‚‰ãªã„ã‹ã‚‰ã§ã™ã€‚ã“ã‚Œã‚’ãƒ’ãƒ³ãƒˆã«ã€ä»Šå›ã®ã‚³ãƒ³ãƒšã§ã‚‚è³ƒæ–™ç·é¡ã§ã¯ãªãã€å¹³ç±³å˜ä¾¡ã®èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚



è³ƒæ–™ç·é¡ãŒéå¸¸ã«é«˜ã„å‹Ÿé›†ã§ã¯ã€å¹³ç±³å˜ä¾¡ãŒé«˜ã„ã®ã‚‚ã•ã‚‹ã“ã¨ãªãŒã‚‰ã€å°‚æœ‰é¢ç©ã‚‚å¤§ãã„ã“ã¨ãŒå¤šã„ã§ã™ã€‚é«˜ã„å¹³ç±³å˜ä¾¡ Ã— å¤§ãã„å°‚æœ‰é¢ç© = éå¸¸ã«é«˜ã„è³ƒæ–™ã€ã¨ãªã£ã¦ã„ã‚‹ã‚ã‘ã§ã™ã€‚è³ƒæ–™ç·é¡ã§ã¯ãªãå¹³ç±³å˜ä¾¡ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ã“ã¨ã§ã€å¤–ã‚Œå€¤ã®å¤–ã‚Œå…·åˆã‚’ç·©å’Œã§ããŸé¢ã‚‚ã‚ã‚Šã¾ã™ã€‚



RMSE ã§ã¯ãªã MAE ã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹éš›ã®è©•ä¾¡æŒ‡æ¨™ã¨ã—ã¦ RMSE ã§ã¯ãªã MAE ã‚’æ¡ç”¨ã—ã¾ã—ãŸã€‚RMSE ã¯èª¤å·®ã‚’äºŒä¹—ã—ã¦å¹³å‡ã‚’å–ã‚‹ãŸã‚ã€10ä¸‡å††ã®èª¤å·®1ä»¶ãŒ1ä¸‡å††ã®èª¤å·®100ä»¶ã¨åŒã˜å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€å¤–ã‚Œå€¤ã«å¤§ããå¼•ã£å¼µã‚‰ã‚Œã‚„ã™ã„æŒ‡æ¨™ã§ã™ã€‚ã“ã®ã‚ˆã†ãªæ€§è³ªã‹ã‚‰ã€é«˜ç´šè³ƒè²¸ç‰©ä»¶ã«æœ€é©åŒ–ã•ã‚Œã€å¤§å¤šæ•°ã®é€šå¸¸ç‰©ä»¶ã«å¯¾ã™ã‚‹äºˆæ¸¬ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã—ãŸã€‚



ä¸€æ–¹ã§ã€MAEã¯èª¤å·®ã‚’çµ¶å¯¾å€¤ã¨ã—ã¦è©•ä¾¡ã™ã‚‹ãŸã‚ã€å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚é€šå¸¸ã®è³ƒè²¸ç‰©ä»¶ã§é©åˆ‡ãªäºˆæ¸¬ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€é«˜ç´šè³ƒè²¸ã¸ã®ä¾å­˜åº¦ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ã‚’ç›®çš„ã«ã€MAEã‚’é¸æŠã—ã¾ã—ãŸã€‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«é«˜ç´šè³ƒè²¸ç‰©ä»¶ãŒå¤šãå«ã¾ã‚Œã¦ã„ã‚‹ã¨å±ãªã„æ°—ãŒã—ã¾ã™ãŒã€RMSE ã§å­¦ç¿’ã•ã›ãŸã¨ã—ã¦ã‚‚ç²¾åº¦é«˜ãå½“ã¦ã«ã„ãã®ã¯é›£ã—ã„ã¨æ€ã„ã€å‰²ã‚Šåˆ‡ã‚Šã¾ã—ãŸã€‚



æ¬ æã—ã¦ã„ã‚‹é¢ç©ã‚’åŸ‹ã‚ã‚‹

å…ˆã«è¿°ã¹ãŸã‚ˆã†ã«å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«ã—ãŸãŸã‚ã€å°‚æœ‰é¢ç©ãŒç›´æ¥ã‚¹ã‚³ã‚¢ã«é–¢ã‚ã£ã¦ãã‚‹é‡è¦ãªå€¤ã¨ãªã‚Šã¾ã—ãŸãŒã€ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã¯å°‚æœ‰é¢ç©ãŒæ¬ æã—ã¦ã„ã¾ã—ãŸã€‚ãã“ã§ã€ã¾ãšå°‚æœ‰é¢ç©äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’ä½œã‚Šã€ã“ã‚Œã‚’ä½¿ã£ã¦å°‚æœ‰é¢ç©ã‚’åŸ‹ã‚ã¦ã‹ã‚‰ã€å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚å°‚æœ‰é¢ç©äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¯å¹³ç±³å˜ä¾¡äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã»ã¼åŒã˜ LightGBM ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚



æ¬ æã—ã¦ã„ã‚‹éƒµä¾¿ç•ªå·ã‚’åŸ‹ã‚ã‚‹

å¾Œè¿°ã®é€šã‚Šéƒµä¾¿ç•ªå·ãŒéå¸¸ã«å¼·ã„ç‰¹å¾´é‡ã«ãªã£ã¦ã„ã¾ã—ãŸãŒã€ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã¯éƒµä¾¿ç•ªå·ãŒæ¬ æã—ã¦ã„ãŸã®ã§ã€ã‚¼ãƒ³ãƒªãƒ³ API ã‚’ä½¿ã£ã¦éƒµä¾¿ç•ªå·ã‚’åŸ‹ã‚ã¾ã—ãŸã€‚



åŒä¸€éƒ¨å±‹ã®è³ƒæ–™ã‹ã‚‰è³ƒæ–™ã‚’æ±ºã‚æ‰“ã¡ã™ã‚‹

test ã¨åŒã˜éƒ¨å±‹ã®åˆ¥ã®æ™‚æœŸã®ãƒ‡ãƒ¼ã‚¿ãŒ train ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚Šã¾ã—ãŸã€‚éƒ½å¿ƒã®è³ƒæ–™ã¯ä¸Šæ˜‡å‚¾å‘ã«ã‚ã‚‹ä¸€æ–¹ã§éƒŠå¤–ã¯ä¸‹è½å‚¾å‘ã«ã‚ã£ãŸã‚Šã€ã‚³ãƒ­ãƒŠã§è½ã¡è¾¼ã‚“ã§ã„ã‚‹æ™‚æœŸãŒã‚ã£ãŸã‚Šã€åŒã˜éƒ¨å±‹ã§ã‚‚æ™‚æœŸã«ã‚ˆã£ã¦è³ƒæ–™ã¯ç•°ãªã‚Šã¾ã™ã€‚å‹Ÿé›†æ™‚æœŸã‚’å¤‰æ•°ã«ç·šå½¢å›å¸°ã—ãŸè³ƒæ–™ã‚’ä½¿ã„ã¾ã—ãŸã€‚



åŒä¸€ãƒ“ãƒ«ã®è³ƒæ–™ã‹ã‚‰è³ƒæ–™ã‚’æ±ºã‚æ‰“ã¡ã™ã‚‹

test ã¨åŒã˜å»ºç‰©ã®ãƒ‡ãƒ¼ã‚¿ãŒ train ã«ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ãŒå¤šæ•°ã‚ã‚Šã¾ã—ãŸã€‚å½“åˆã¯ã€åŒã˜ç‰©ä»¶ãªã‚‰å¹³ç±³å˜ä¾¡ã‚‚åŒã˜ã¨è€ƒãˆã¦ train ã®ãƒ‡ãƒ¼ã‚¿ã§ test ã‚’åŸ‹ã‚ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€train ã®ä¸­ã§ã‚‚åŒä¸€ç‰©ä»¶å†…ã§å¹³ç±³å˜ä¾¡ãŒå¤§ããç•°ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒã„ãã¤ã‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ãã“ã§ã€å¹³ç±³å˜ä¾¡ã‚’ãã®ã¾ã¾ä½¿ã†ã®ã§ã¯ãªãã€å°‚æœ‰é¢ç©ãƒ»éšæ•°ãƒ»å‹Ÿé›†æ™‚æœŸã§ç·šå½¢å›å¸°ã—ãŸå€¤ã‚’ä½¿ã„ã¾ã—ãŸã€‚



å‡ºåŠ›ã‚’è¦‹ã¦ã¿ã‚‹ã¨æ¥µç«¯ãªå€¤ãŒã„ãã¤ã‹ã‚ã£ãŸã®ã§ã€é©å½“ãªé–¾å€¤ã‚’è¶…ãˆã‚‹å€¤ã¯ä½¿ã‚ãªã„ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚



ç‰¹å¾´é‡ã‚’ä½œã‚‹

ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ã£ãŸã‚Šã€é›†è¨ˆã—ãŸã‚Šã—ãŸä»–ã€å›½åœŸæ•°å€¤æƒ…å ±ã‚„ã‚¼ãƒ³ãƒªãƒ³ API ã¨çµ„ã¿åˆã‚ã›ã¦427å€‹ã®ç‰¹å¾´é‡ã‚’ä½œã‚Šã¾ã—ãŸã€‚RMSE ã§ã®è©•ä¾¡å€¤ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã€å¤–ã‚Œå€¤ã§ã‚ã‚‹é«˜ç´šè³ƒè²¸ã«ç‰¹åŒ–ã—ãŸç‰¹å¾´é‡ã‚‚å¤šãã‚ã‚Šã¾ã™ã€‚



ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä½œã‚‹

ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä½¿ã£ãŸã‚‚ã®ã‚‚ã‚ã‚Šã¾ã™ãŒã€ãã®ä¸­ã§ã‚‚æ¬¡ã®ç‰¹å¾´é‡ã¯ç‰¹ã«å¼·ã‹ã£ãŸã§ã™ã€‚



å°‚æœ‰é¢ç©

ãƒ‡ãƒ¼ã‚¿ã‚’ã„ãã¤ã‹è¦‹ã¦ã¿ãŸã¨ã“ã‚ã€éƒŠå¤–ã§ã¯å°‚æœ‰é¢ç©ãŒå¤§ãããªã‚‹ã¨å¹³ç±³å˜ä¾¡ã¯ä¸‹ãŒã‚‹å‚¾å‘ãŒã‚ã‚‹ä¸€æ–¹ã§ã€éƒ½å¿ƒã§ã¯å°‚æœ‰é¢ç©ãŒå¤§ãããªã‚‹ã¨å¹³ç±³å˜ä¾¡ã‚‚ä¸ŠãŒã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã—ãŸã€‚

ã‚³ãƒ³ãƒšçµ‚äº†å¾Œã«ç¤¾å†…ã®æœ‰è­˜è€…ã«èã„ã¦ã¿ãŸã¨ã“ã‚ã€éƒ½å¿ƒã®åºƒã„è³ƒè²¸ãƒãƒ³ã‚·ãƒ§ãƒ³ã¯å¯Œè£•å±¤ã‹ã‚‰ã®éœ€è¦ãŒé«˜ããªã‚‹ãŸã‚ã€åºƒã„éƒ¨å±‹ã»ã©å¹³ç±³å˜ä¾¡ãŒä¸ŠãŒã‚‹å‚¾å‘ãŒã‚ã‚‹ã¨ã®ã“ã¨ã§ã—ãŸã€‚

é–“å–ã‚Šã‚’æ•°å€¤åŒ–ã—ãŸã‚‚ã®

1K < 1DK < 1LDK < 2K < â€¦ ã®ã‚ˆã†ã«é–“å–ã‚Šã«åºåˆ—ã‚’ã¤ã‘ã¦æ•°å€¤åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚

ç¯‰å¹´æ•°

å¸‚åŒºç”ºæ‘å

éƒµä¾¿ç•ªå·

åˆæœŸã‹ã‚‰éå¸¸ã«å¼·ã„ç‰¹å¾´é‡ã§ã—ãŸãŒã€ãªãœã‹ã¯æœ€å¾Œã¾ã§åˆ†ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚éƒµä¾¿ç•ªå·ã§è³ƒè²¸ã‚’é¸ã¶ã“ã¨ã¯ãªã„ã¨æ€ã†ã®ã§ã€è£ã«éš ã‚ŒãŸä½•ã‚‰ã‹ã®åœ°åŸŸæ€§ãŒé‡è¦ã ã¨ã¯æ€ã†ã®ã§ã™ãŒâ€¦â€¦

ã¾ãŸã€ä¸ãˆã‚‰ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ–°ã—ãç‰¹å¾´é‡ã‚’ä½œã‚Šã¾ã—ãŸã€‚ãã®ä¸­ã§ã‚‚ç‰¹ã«å¼·ã‹ã£ãŸç‰¹å¾´é‡ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚



åŒä¸€éƒµä¾¿ç•ªå·å†…ã§ã®å¹³ç±³å˜ä¾¡ã®å¹³å‡å€¤ãƒ»åˆ†æ•£

å»ºç‰©å‘¨è¾ºã®å‹Ÿé›†ã®å¹³ç±³å˜ä¾¡ã®å¹³å‡å€¤ã‚’ã€Œç›¸å ´ã€ã¨ã—ã¦ä½¿ãˆã‚‹ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã¾ã—ãŸã€‚

éƒµä¾¿ç•ªå·ã§ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã—ãŸã®ã¯ã€å‰è¿°ã®é€šã‚Šãªãœã‹éƒµä¾¿ç•ªå·å˜ä½“ãŒéå¸¸ã«å¼·ã„ç‰¹å¾´é‡ã ã£ãŸãŸã‚ã§ã™ã€‚

å‘¨å›² 2 km ä»¥å†…ã®å¹³ç±³å˜ä¾¡ã®å¹³å‡å€¤ãƒ»åˆ†æ•£

å»ºç‰©å‘¨è¾ºã®å‹Ÿé›†ã®å¹³ç±³å˜ä¾¡ã®å¹³å‡å€¤ã‚’ã€Œç›¸å ´ã€ã¨ã—ã¦ä½¿ã£ã¦ã„ã¾ã™ã€‚2 kmã¨ã„ã†é–¾å€¤ã¯é©å½“ã§ã€åœ°å›³ã‚’è¦‹ãŸã¨ãã«è‡ªåˆ†ãŒæ•£æ­©ã™ã‚‹ç¯„å›²ãŒè‡ªå®…ã®åŠå¾„ 2 km ä»¥å†…ã ã£ãŸãŸã‚ã§ã™ã€‚

ç¯‰å¹´æ•°ãŒåŒä¸€å¸‚åŒºç”ºæ‘å†…ã®å¹³å‡ã®ä½•å€ã‹

ç¯‰å¹´æ•°ã¯è³ƒè²¸ã‚’é¸ã¶ã¨ãã«ã‚ˆãè¦‹ã‚‹æŒ‡æ¨™ã®ä¸€ã¤ã§ã™ãŒã€å¤ã„å»ºç‰©ãŒå¤šã„åœ°åŸŸã§ã®ç¯‰æµ…ã®ç‰©ä»¶ã¯ä¾¡å€¤ãŒé«˜ããªã‚‹ã¨è€ƒãˆã€ç‰¹å¾´é‡ã«ã—ã¾ã—ãŸã€‚

å°‚æœ‰é¢ç©ãŒåŒä¸€å¸‚åŒºç”ºæ‘å†…ã®å¹³å‡ã®ä½•å€ã‹

éƒ½å¿ƒã§å°‚æœ‰é¢ç©ãŒå¤§ãã„ç‰©ä»¶ã¯å¸Œå°‘ä¾¡å€¤ãŒé«˜ã„ã®ã§ã€é«˜ç´šè³ƒè²¸ã‚’å½“ã¦ã‚‹æŒ‡æ¨™ã«ãªã‚‹ã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã€ç‰¹å¾´é‡ã«ã—ã¾ã—ãŸã€‚

ç”ºå

åŒã˜æ¸¯åŒºã§ã‚‚ä½æ‰€ãŒæ–°æ©‹ã‹å…­æœ¬æœ¨ã‹ã§è³ƒæ–™ãŒå¤‰ã‚ã‚Šãã†ã ã¨æ€ã£ãŸã®ã§å…¥ã‚Œã¾ã—ãŸã€‚

å›½åœŸæ•°å€¤æƒ…å ±

ç‰¹å¾´é‡ã®ä½œæˆã«å›½åœŸæ•°å€¤æƒ…å ±ã‚’æ´»ç”¨ã—ã¾ã—ãŸã€‚åˆ©ç”¨ã—ãŸå›½åœŸæ•°å€¤æƒ…å ±ã¯ä»¥ä¸‹ã®10å€‹ã§ã™ã€‚



åœ°ä¾¡å…¬ç¤º

ã€€a. åœ°ä¾¡ã ã‘ã§ã‚‚å¼·ã„ç‰¹å¾´é‡ã ã‘ã§ã—ãŸãŒã€ç›´è¿‘ã®ã‚‚ã®ã ã‘ã§ãªãéå»ã®å¹´åº¦ã®å€¤ã‚‚ã‚ã‚‹ã®ã§ã€å¤‰å‹•ç‡ã®ã‚ˆã†ãªç‰¹å¾´é‡ã‚‚ä½œã‚‹ã“ã¨ãŒã§ãã€ç²¾åº¦æ”¹å–„ã«ç¹‹ãŒã‚Šã¾ã—ãŸã€‚



éƒ½é“åºœçœŒåœ°ä¾¡èª¿æŸ»

ã€€a. ã“ã¡ã‚‰ã‚‚å¼·ã„ç‰¹å¾´é‡ã§ã—ãŸã€‚åœ°ä¾¡å…¬ç¤ºã¨ã¯èª¿æŸ»ä¸»ä½“ãŒç•°ãªã‚‹ã®ã§ã€åœ°ä¾¡ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã£ãŸã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€‚



å°å­¦æ ¡åŒº

ã€€a. éƒ½å¿ƒã§ã¯ä¸­å­¦å—é¨“ã«å‚™ãˆã¦æœ‰åå…¬ç«‹å°å­¦æ ¡ã«å­ã©ã‚‚ã‚’å…¥ã‚ŒãŸã„ã‚‰ã—ã„ã€ã¨ã„ã†è©±ã‚’èã„ãŸã®ã§å…¥ã‚Œã¾ã—ãŸã€‚



é§…åˆ¥ä¹—é™å®¢æ•°

ã€€a. æœ€å¯„ã‚Šé§…ã®ä¹—é™å®¢æ•°ã¨ã—ã¦å…¥ã‚Œã‚‹ã“ã¨ã§ã€åœ°åŸŸã®æ€§è³ªã‚’è¡¨ç¾ã§ãã‚‹ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã¾ã—ãŸã€‚

ã€€b. é§…è‡ªä½“ã®æƒ…å ±ã‚‚å…¥ã£ã¦ã„ã‚‹ã®ã§ã€æœ€å¯„ã‚Šé§…ã¸ã®è·é›¢ã‚’å‡ºã™ã“ã¨ã‚‚ã§ãã¾ã—ãŸã€‚

ã€€c. ä¹—é™å®¢æ•°ä¸Šä½25ã®é§…ã‹ã‚‰ã®è·é›¢ã‚’å‡ºã™ã“ã¨ã§ã€ã€Œéƒ½ä¼šåº¦ã€ã‚’è¡¨ç¾ã™ã‚‹ç‰¹å¾´é‡ã‚‚ä½œã‚Šã¾ã—ãŸã€‚



åŒ»ç™‚æ©Ÿé–¢

ã€€a. å°å…ç§‘ãŒè¿‘ã„ç‰©ä»¶ã¯ãƒ•ã‚¡ãƒŸãƒªãƒ¼éœ€è¦ã‹ã‚‰è³ƒæ–™ã‚‚é«˜ããªã‚‹ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã€æœ€å¯„ã‚Šã®å°å…ç§‘ã¸ã®è·é›¢ã‚’ç®—å‡ºã™ã‚‹ã®ã«ä½¿ã„ã¾ã—ãŸã€‚



1kmãƒ¡ãƒƒã‚·ãƒ¥åˆ¥å°†æ¥æ¨è¨ˆäººå£

ã€€a. ä¼šç¤¾ã§äº‹æ¥­é–‹ç™ºã®äººãŒã€Œäººå£ã¯æœ€ã‚‚å˜˜ã‚’ã¤ã‹ãªã„æŒ‡æ¨™ã€ã¨è¨€ã£ã¦ã„ãŸã®ã‚’èã„ã¦å…¥ã‚Œã¾ã—ãŸã€‚

ã€€b. å…¨ä½“çš„ã«å¼·ãã€ç‰¹ã« 35ã€œ39 æ­³ã®äººå£ã€ 45ã€œ49 æ­³ã®äººå£ã€ 5ã€œ9 æ­³ã®äººå£ã®3ã¤ãŒå¼·ã‹ã£ãŸã§ã™ã€‚



å¹³å¹´å€¤ï¼ˆæ°—å€™ï¼‰ãƒ¡ãƒƒã‚·ãƒ¥

ã€€a. éã”ã—ã‚„ã™ã„å ´æ‰€ã»ã©è³ƒæ–™ãŒé«˜ã„ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã¾ã—ãŸã€‚

ã€€b. æ—¥ç…§æ™‚é–“ã‚„æœ€é«˜æ°—æ¸©ãŒç‰¹å¾´é‡ã¨ã—ã¦å¼·ã‹ã£ãŸã§ã™ã€‚



ç”¨é€”åœ°åŸŸ

ã€€a. ç¬¬ä¸€ç¨®ä½å±¤ä½å±…å°‚ç”¨åœ°åŸŸã«ã¯ã‚³ãƒ³ãƒ“ãƒ‹ãªã©ã®åº—èˆ—ãŒä½œã‚Œãªã„ãŒã€ç¬¬äºŒç¨®ä½å±¤ä½å±…å°‚ç”¨åœ°åŸŸã«ã¯ä½œã‚Œã‚‹ã€ã¿ãŸã„ãªæƒ…å ±ãŒåŠ¹ãã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã€å…¥ã‚Œã¾ã—ãŸã€‚ãã“ãã“å¼·ã„ç‰¹å¾´é‡ã§ã—ãŸã€‚



äººå£é›†ä¸­åœ°åŒºãƒ‡ãƒ¼ã‚¿

ã€€a. åœ°åŸŸã®æ€§è³ªã‚’è¡¨ç¾ã§ãã‚‹ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã¾ã—ãŸã€‚äººå£å¯†åº¦ã®å€¤ãŒåŠ¹ã„ã¦ã„ã¾ã—ãŸã€‚



å­¦æ ¡

ã€€a. æœ€å¯„ã‚Šã®ä¿è‚²åœ’ã¾ã§ã®è·é›¢ã‚’å…¥ã‚Œã‚‹ã®ã«ä½¿ã„ã¾ã—ãŸã€‚



ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã¨ã—ã¦ç”¨ã„ã‚‰ã‚Œã‚‹æ´ªæ°´æµ¸æ°´æƒ³å®šåŒºåŸŸãƒ‡ãƒ¼ã‚¿ã‚‚ä½¿ãˆã‚Œã°è‰¯ã‹ã£ãŸã®ã§ã™ãŒã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã«æ¯”ã¹ã¦ã‚µã‚¤ã‚ºãŒæ¡é•ã„ã«å¤§ãã‹ã£ãŸã®ã§ã€å¾Œå›ã—ã«ã—ã¦ã„ã‚‹ã†ã¡ã«çµ‚ã‚ã£ã¦ã—ã¾ã„ã¾ã—ãŸã€‚



ã‚¼ãƒ³ãƒªãƒ³ API

ã‚¼ãƒ³ãƒªãƒ³ API ã‚’ç”¨ã„ã¦å…¨å›½ã®ã‚³ãƒ³ãƒ“ãƒ‹ä¸€è¦§ã‚’å…¥æ‰‹ã—ã€æœ€å¯„ã‚Šã®ã‚³ãƒ³ãƒ“ãƒ‹ã¾ã§ã®è·é›¢ã‚’ç‰¹å¾´é‡ã¨ã—ã¦å…¥ã‚Œã¾ã—ãŸã€‚



1ä½è§£æ³•ã‚’æ”¯ãˆã‚‹æŠ€è¡“

å€‹äººçš„ã«ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã®ç«¶æŠ€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯å¥½ããªã®ã§ã™ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ã‚³ãƒ³ãƒšã«ã¯è‹¦æ‰‹æ„è­˜ãŒã‚ã‚Šã¾ã—ãŸã€‚éå»ã«å‚åŠ ã—ãŸã‚³ãƒ³ãƒšã§ã¯ã€notebook ã‚„ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæ•£ä¹±ã—ã€ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆã‚„ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå†ç¾ã§ããªããªã‚Šã€ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¤±ã£ã¦ã„ãã“ã¨ãŒå¤šã€…ã‚ã£ãŸãŸã‚ã§ã™ã€‚



ä»Šå›ã¯è‹¦æ‰‹æ„è­˜ã‚’æ‰•æ‹­ã™ã¹ãã€ãƒ‡ãƒ¼ã‚¿ã‚„å­¦ç¿’ã®å†ç¾æ€§ã‚’æ‹…ä¿ã—ã¦ã€ç¶™ç¶šçš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã§ãã‚‹é–‹ç™ºç’°å¢ƒã‚’ä½œã‚‹ã“ã¨ã«é‡ç‚¹ã‚’ç½®ãã¾ã—ãŸã€‚ã“ã®ã‚³ãƒ³ãƒšã§ã¯å›½åœŸæ•°å€¤æƒ…å ±ã‚„ã‚¼ãƒ³ãƒªãƒ³ãªã©ã®å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®è¨­è¨ˆã‚’é ‘å¼µã‚‹ã‚ˆã‚Šã‚‚ã€ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é ‘å¼µã£ã¦è‰¯ã„ç‰¹å¾´é‡ã‚’ä½œã‚‹ã“ã¨ãŒé‡è¦ã«ãªã‚‹ã¨è€ƒãˆãŸã®ã‚‚ã‚ã‚Šã¾ã™ã€‚

```

---

**Cursor**

ç´ æ™´ã‚‰ã—ã„è¨˜äº‹ã§ã™ã­ï¼éå¸¸ã«å‚è€ƒã«ãªã‚‹ãƒã‚¤ãƒ³ãƒˆãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™ã€‚ã“ã®è§£æ³•ã‚’ä»Šå›ã®å£²è²·ä¾¡æ ¼äºˆæ¸¬ã«é©ç”¨ã§ãã‚‹æ”¹å–„ç‚¹ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

## ä¸»è¦ãªæ”¹å–„ãƒã‚¤ãƒ³ãƒˆ

### 1. **å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹** ğŸ“Š
è¨˜äº‹ã®æœ€ã‚‚é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚`money_room`ï¼ˆå£²è²·ä¾¡æ ¼ï¼‰ã‚’ç›´æ¥äºˆæ¸¬ã™ã‚‹ã®ã§ã¯ãªãã€`money_room / unit_area`ï¼ˆå¹³ç±³å˜ä¾¡ï¼‰ã‚’äºˆæ¸¬ã—ã€æœ€å¾Œã«å°‚æœ‰é¢ç©ã‚’æ›ã‘ã¦æˆ»ã—ã¾ã™ã€‚

### 2. **MAEï¼ˆL1ï¼‰ã‚’ä½¿ç”¨** 
MAPEã®ä»£ã‚ã‚Šã«MAEã‚’å­¦ç¿’æŒ‡æ¨™ã«ã™ã‚‹ã“ã¨ã§å¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’æŠ‘åˆ¶ã€‚

### 3. **åŒä¸€å»ºç‰©ã®æƒ…å ±ã‚’æ´»ç”¨**
`building_id`ãŒåŒã˜ç‰©ä»¶ã®trainãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¾¡æ ¼æƒ…å ±ã‚’æ´»ç”¨ã€‚

### 4. **éƒµä¾¿ç•ªå·ã®æ´»ç”¨**
`post1`, `post2`ï¼ˆéƒµä¾¿ç•ªå·ï¼‰ã¯éå¸¸ã«å¼·ã„ç‰¹å¾´é‡ã¨ã®ã“ã¨ã€‚

### 5. **é‡è¦ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
- åŒä¸€éƒµä¾¿ç•ªå·å†…ã®å¹³ç±³å˜ä¾¡å¹³å‡ãƒ»åˆ†æ•£
- åŒä¸€å¸‚åŒºç”ºæ‘å†…ã§ã®ç¯‰å¹´æ•°ãƒ»é¢ç©ã®ç›¸å¯¾å€¤
- å‘¨å›²ã®ç›¸å ´æƒ…å ±

---

## æ”¹å–„ç‰ˆã‚³ãƒ¼ãƒ‰

Agentãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ä»¥ä¸‹ã®æ”¹å–„ã‚’é©ç”¨ã§ãã¾ã™ï¼š

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ
å‚è€ƒ: ç¬¬1å›åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1ä½è§£æ³•
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test

def create_target_features(train, test):
    """
    å¹³ç±³å˜ä¾¡ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ & åŒä¸€å»ºç‰©ã®æƒ…å ±ã‚’æ´»ç”¨
    """
    print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # å°‚æœ‰é¢ç©ã®æ¬ æã‚’åŸ‹ã‚ã‚‹ï¼ˆä¸­å¤®å€¤ã§ï¼‰
    unit_area_median = train['unit_area'].median()
    train['unit_area'] = train['unit_area'].fillna(unit_area_median)
    test['unit_area'] = test['unit_area'].fillna(unit_area_median)
    
    # å¹³ç±³å˜ä¾¡ã‚’è¨ˆç®—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    train['price_per_sqm'] = train['money_room'] / train['unit_area'].replace(0, np.nan)
    train['price_per_sqm'] = train['price_per_sqm'].fillna(train['price_per_sqm'].median())
    
    # åŒä¸€å»ºç‰©(building_id)ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆã‚’trainã‹ã‚‰è¨ˆç®—
    building_stats = train.groupby('building_id').agg({
        'price_per_sqm': ['mean', 'std', 'min', 'max', 'count'],
        'money_room': ['mean', 'std']
    }).reset_index()
    building_stats.columns = ['building_id', 'bld_price_sqm_mean', 'bld_price_sqm_std', 
                               'bld_price_sqm_min', 'bld_price_sqm_max', 'bld_count',
                               'bld_price_mean', 'bld_price_std']
    
    train = train.merge(building_stats, on='building_id', how='left')
    test = test.merge(building_stats, on='building_id', how='left')
    
    return train, test

def create_location_features(train, test):
    """
    éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    """
    print("ä½ç½®æƒ…å ±ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # éƒµä¾¿ç•ªå·(post1)ã”ã¨ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆ
    post_stats = train.groupby('post1').agg({
        'price_per_sqm': ['mean', 'std', 'median'],
        'money_room': ['mean', 'median']
    }).reset_index()
    post_stats.columns = ['post1', 'post_price_sqm_mean', 'post_price_sqm_std', 
                          'post_price_sqm_median', 'post_price_mean', 'post_price_median']
    
    train = train.merge(post_stats, on='post1', how='left')
    test = test.merge(post_stats, on='post1', how='left')
    
    # å¸‚åŒºç”ºæ‘(addr1_1 + addr1_2)ã”ã¨ã®çµ±è¨ˆ
    for df in [train, test]:
        df['city_code'] = df['addr1_1'].astype(str) + '_' + df['addr1_2'].astype(str)
    
    city_stats = train.groupby('city_code').agg({
        'price_per_sqm': ['mean', 'std'],
        'year_built': ['mean'],
        'unit_area': ['mean']
    }).reset_index()
    city_stats.columns = ['city_code', 'city_price_sqm_mean', 'city_price_sqm_std',
                          'city_year_built_mean', 'city_unit_area_mean']
    
    train = train.merge(city_stats, on='city_code', how='left')
    test = test.merge(city_stats, on='city_code', how='left')
    
    # ç¯‰å¹´æ•°ãƒ»é¢ç©ã®åœ°åŸŸå¹³å‡ã¨ã®æ¯”
    for df in [train, test]:
        df['year_built_ratio'] = df['year_built'] / df['city_year_built_mean'].replace(0, np.nan)
        df['unit_area_ratio'] = df['unit_area'] / df['city_unit_area_mean'].replace(0, np.nan)
    
    return train, test

def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆå¹³ç±³å˜ä¾¡ï¼‰
    y_train = train['price_per_sqm'].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = ['money_room', 'price_per_sqm', 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses', 'city_code']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols

def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆMAEä½¿ç”¨ + seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    n_splits = 5
    seeds = [42, 123, 456]  # è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    all_models = []
    
    for seed in seeds:
        print(f"\n=== Seed: {seed} ===")
        
        params = {
            'objective': 'regression',
            'metric': 'mae',  # MAEã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
            'boosting_type': 'gbdt',
            'learning_rate': 0.05,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 20,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'n_jobs': -1,
            'seed': seed
        }
        
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        
        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
            print(f"Fold {fold + 1}/{n_splits}")
            
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train[train_idx], y_train[valid_idx]
            
            cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
            
            train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
            valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=3000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(period=500)
                ]
            )
            
            all_models.append(model)
    
    print(f"\nå­¦ç¿’å®Œäº†: {len(all_models)}ãƒ¢ãƒ‡ãƒ«")
    return all_models

def predict_and_submit(models, X_test, test_unit_area, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬
    price_per_sqm_pred = np.zeros(len(X_test))
    for model in models:
        price_per_sqm_pred += model.predict(X_test)
    price_per_sqm_pred /= len(models)
    
    # å¹³ç±³å˜ä¾¡ Ã— å°‚æœ‰é¢ç© = å£²è²·ä¾¡æ ¼
    predictions = price_per_sqm_pred * test_unit_area
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v2.csv'
    submission.to_csv(output_path, index=False, header=False)
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    
    return submission

def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã¨å°‚æœ‰é¢ç©ã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # å¹³ç±³å˜ä¾¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ & åŒä¸€å»ºç‰©æƒ…å ±
    train, test = create_target_features(train, test)
    
    # ä½ç½®æƒ…å ±ç‰¹å¾´é‡
    train, test = create_location_features(train, test)
    
    # å°‚æœ‰é¢ç©ã‚’ä¿å­˜ï¼ˆæœ€å¾Œã«æ›ã‘ç®—ã™ã‚‹ï¼‰
    test_unit_area = test['unit_area'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_unit_area, test_ids)
    
    print("\n=== å‡¦ç†å®Œäº† ===")
    return submission

if __name__ == '__main__':
    main()
```

---

## ä¸»ãªæ”¹å–„ç‚¹ã¾ã¨ã‚

| æ”¹å–„é …ç›® | å¤‰æ›´å†…å®¹ |
|---------|---------|
| ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ | `money_room` â†’ `price_per_sqm`ï¼ˆå¹³ç±³å˜ä¾¡ï¼‰ |
| æå¤±é–¢æ•° | MAPE â†’ MAEï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰ |
| åŒä¸€å»ºç‰©æƒ…å ± | `building_id`ã§é›†è¨ˆã—ãŸä¾¡æ ¼çµ±è¨ˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ  |
| éƒµä¾¿ç•ªå·æ´»ç”¨ | `post1`ã”ã¨ã®ä¾¡æ ¼çµ±è¨ˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ  |
| åœ°åŸŸç›¸å ´ | å¸‚åŒºç”ºæ‘ã”ã¨ã®ç¯‰å¹´æ•°ãƒ»é¢ç©å¹³å‡ã¨ã®æ¯”ã‚’è¿½åŠ  |
| ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« | è¤‡æ•°seedï¼ˆ42, 123, 456ï¼‰ã®ãƒ¢ãƒ‡ãƒ«å¹³å‡ |

---

é©ç”¨ã—ãŸã„å ´åˆã¯ **Agentãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¦** ãã ã•ã„ï¼

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ
å‚è€ƒ: ç¬¬1å›åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1ä½è§£æ³•
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'

def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test

def create_target_features(train, test):
    """
    å¹³ç±³å˜ä¾¡ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ & åŒä¸€å»ºç‰©ã®æƒ…å ±ã‚’æ´»ç”¨
    """
    print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # å°‚æœ‰é¢ç©ã®æ¬ æã‚’åŸ‹ã‚ã‚‹ï¼ˆä¸­å¤®å€¤ã§ï¼‰
    unit_area_median = train['unit_area'].median()
    train['unit_area'] = train['unit_area'].fillna(unit_area_median)
    test['unit_area'] = test['unit_area'].fillna(unit_area_median)
    
    # å¹³ç±³å˜ä¾¡ã‚’è¨ˆç®—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    train['price_per_sqm'] = train['money_room'] / train['unit_area'].replace(0, np.nan)
    train['price_per_sqm'] = train['price_per_sqm'].fillna(train['price_per_sqm'].median())
    
    # åŒä¸€å»ºç‰©(building_id)ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆã‚’trainã‹ã‚‰è¨ˆç®—
    building_stats = train.groupby('building_id').agg({
        'price_per_sqm': ['mean', 'std', 'min', 'max', 'count'],
        'money_room': ['mean', 'std']
    }).reset_index()
    building_stats.columns = ['building_id', 'bld_price_sqm_mean', 'bld_price_sqm_std', 
                               'bld_price_sqm_min', 'bld_price_sqm_max', 'bld_count',
                               'bld_price_mean', 'bld_price_std']
    
    train = train.merge(building_stats, on='building_id', how='left')
    test = test.merge(building_stats, on='building_id', how='left')
    
    return train, test

def create_location_features(train, test):
    """
    éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    """
    print("ä½ç½®æƒ…å ±ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # éƒµä¾¿ç•ªå·(post1)ã”ã¨ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆ
    post_stats = train.groupby('post1').agg({
        'price_per_sqm': ['mean', 'std', 'median'],
        'money_room': ['mean', 'median']
    }).reset_index()
    post_stats.columns = ['post1', 'post_price_sqm_mean', 'post_price_sqm_std', 
                          'post_price_sqm_median', 'post_price_mean', 'post_price_median']
    
    train = train.merge(post_stats, on='post1', how='left')
    test = test.merge(post_stats, on='post1', how='left')
    
    # å¸‚åŒºç”ºæ‘(addr1_1 + addr1_2)ã”ã¨ã®çµ±è¨ˆ
    for df in [train, test]:
        df['city_code'] = df['addr1_1'].astype(str) + '_' + df['addr1_2'].astype(str)
    
    city_stats = train.groupby('city_code').agg({
        'price_per_sqm': ['mean', 'std'],
        'year_built': ['mean'],
        'unit_area': ['mean']
    }).reset_index()
    city_stats.columns = ['city_code', 'city_price_sqm_mean', 'city_price_sqm_std',
                          'city_year_built_mean', 'city_unit_area_mean']
    
    train = train.merge(city_stats, on='city_code', how='left')
    test = test.merge(city_stats, on='city_code', how='left')
    
    # ç¯‰å¹´æ•°ãƒ»é¢ç©ã®åœ°åŸŸå¹³å‡ã¨ã®æ¯”
    for df in [train, test]:
        df['year_built_ratio'] = df['year_built'] / df['city_year_built_mean'].replace(0, np.nan)
        df['unit_area_ratio'] = df['unit_area'] / df['city_unit_area_mean'].replace(0, np.nan)
    
    return train, test

def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆå¹³ç±³å˜ä¾¡ï¼‰
    y_train = train['price_per_sqm'].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = ['money_room', 'price_per_sqm', 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses', 'city_code']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols

def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆMAEä½¿ç”¨ + seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    n_splits = 5
    seeds = [42, 123, 456]  # è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    all_models = []
    
    for seed in seeds:
        print(f"\n=== Seed: {seed} ===")
        
        params = {
            'objective': 'regression',
            'metric': 'mae',  # MAEã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
            'boosting_type': 'gbdt',
            'learning_rate': 0.05,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 20,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'n_jobs': -1,
            'seed': seed
        }
        
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        
        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
            print(f"Fold {fold + 1}/{n_splits}")
            
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train[train_idx], y_train[valid_idx]
            
            cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
            
            train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
            valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=3000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(period=500)
                ]
            )
            
            all_models.append(model)
    
    print(f"\nå­¦ç¿’å®Œäº†: {len(all_models)}ãƒ¢ãƒ‡ãƒ«")
    return all_models

def predict_and_submit(models, X_test, test_unit_area, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬
    price_per_sqm_pred = np.zeros(len(X_test))
    for model in models:
        price_per_sqm_pred += model.predict(X_test)
    price_per_sqm_pred /= len(models)
    
    # å¹³ç±³å˜ä¾¡ Ã— å°‚æœ‰é¢ç© = å£²è²·ä¾¡æ ¼
    predictions = price_per_sqm_pred * test_unit_area
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v2.csv'
    submission.to_csv(output_path, index=False, header=False)
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    
    return submission

def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã¨å°‚æœ‰é¢ç©ã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # å¹³ç±³å˜ä¾¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ & åŒä¸€å»ºç‰©æƒ…å ±
    train, test = create_target_features(train, test)
    
    # ä½ç½®æƒ…å ±ç‰¹å¾´é‡
    train, test = create_location_features(train, test)
    
    # å°‚æœ‰é¢ç©ã‚’ä¿å­˜ï¼ˆæœ€å¾Œã«æ›ã‘ç®—ã™ã‚‹ï¼‰
    test_unit_area = test['unit_area'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_unit_area, test_ids)
    
    print("\n=== å‡¦ç†å®Œäº† ===")
    return submission

if __name__ == '__main__':
    main()
```

---

**User**

è¨˜äº‹ã®å†…å®¹ã‚’å…ƒã«æ”¹å–„ç‚¹ã‚’é©ç”¨ã—ã¦ä¸‹ã•ã„ï¼

---

**Cursor**

è¨˜äº‹ã®å†…å®¹ã‚’å…ƒã«æ”¹å–„ç‰ˆã‚³ãƒ¼ãƒ‰ã‚’é©ç”¨ã—ã¾ã™ã€‚

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ
å‚è€ƒ: ç¬¬1å›åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1ä½è§£æ³•

æ”¹å–„ç‚¹:
1. å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ï¼ˆå¤–ã‚Œå€¤ã®å½±éŸ¿ã‚’ç·©å’Œï¼‰
2. MAEï¼ˆL1ï¼‰ã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
3. åŒä¸€å»ºç‰©(building_id)ã®æƒ…å ±ã‚’æ´»ç”¨
4. éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
5. è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'


def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test


def create_target_features(train, test):
    """
    å¹³ç±³å˜ä¾¡ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹ & åŒä¸€å»ºç‰©ã®æƒ…å ±ã‚’æ´»ç”¨
    """
    print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # å°‚æœ‰é¢ç©ã®æ¬ æã‚’åŸ‹ã‚ã‚‹ï¼ˆä¸­å¤®å€¤ã§ï¼‰
    unit_area_median = train['unit_area'].median()
    train['unit_area'] = train['unit_area'].fillna(unit_area_median)
    test['unit_area'] = test['unit_area'].fillna(unit_area_median)
    
    # å°‚æœ‰é¢ç©ãŒ0ã®å ´åˆã‚‚ä¸­å¤®å€¤ã§åŸ‹ã‚ã‚‹
    train.loc[train['unit_area'] <= 0, 'unit_area'] = unit_area_median
    test.loc[test['unit_area'] <= 0, 'unit_area'] = unit_area_median
    
    # å¹³ç±³å˜ä¾¡ã‚’è¨ˆç®—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼‰
    train['price_per_sqm'] = train['money_room'] / train['unit_area']
    
    # åŒä¸€å»ºç‰©(building_id)ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆã‚’trainã‹ã‚‰è¨ˆç®—
    building_stats = train.groupby('building_id').agg({
        'price_per_sqm': ['mean', 'std', 'min', 'max', 'count'],
        'money_room': ['mean', 'std', 'median']
    }).reset_index()
    building_stats.columns = ['building_id', 'bld_price_sqm_mean', 'bld_price_sqm_std', 
                               'bld_price_sqm_min', 'bld_price_sqm_max', 'bld_count',
                               'bld_price_mean', 'bld_price_std', 'bld_price_median']
    
    train = train.merge(building_stats, on='building_id', how='left')
    test = test.merge(building_stats, on='building_id', how='left')
    
    print(f"  - åŒä¸€å»ºç‰©æƒ…å ±: {len(building_stats)}ä»¶ã®building_id")
    
    return train, test


def create_location_features(train, test):
    """
    éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    """
    print("ä½ç½®æƒ…å ±ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # éƒµä¾¿ç•ªå·(post1)ã”ã¨ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆ
    post_stats = train.groupby('post1').agg({
        'price_per_sqm': ['mean', 'std', 'median', 'count'],
        'money_room': ['mean', 'median']
    }).reset_index()
    post_stats.columns = ['post1', 'post_price_sqm_mean', 'post_price_sqm_std', 
                          'post_price_sqm_median', 'post_price_sqm_count',
                          'post_price_mean', 'post_price_median']
    
    train = train.merge(post_stats, on='post1', how='left')
    test = test.merge(post_stats, on='post1', how='left')
    
    print(f"  - éƒµä¾¿ç•ªå·åˆ¥çµ±è¨ˆ: {len(post_stats)}ä»¶")
    
    # å¸‚åŒºç”ºæ‘(addr1_1 + addr1_2)ã”ã¨ã®çµ±è¨ˆ
    for df in [train, test]:
        df['city_code'] = df['addr1_1'].astype(str) + '_' + df['addr1_2'].astype(str)
    
    city_stats = train.groupby('city_code').agg({
        'price_per_sqm': ['mean', 'std', 'median'],
        'year_built': ['mean'],
        'unit_area': ['mean'],
        'money_room': ['mean', 'median']
    }).reset_index()
    city_stats.columns = ['city_code', 'city_price_sqm_mean', 'city_price_sqm_std',
                          'city_price_sqm_median', 'city_year_built_mean', 
                          'city_unit_area_mean', 'city_price_mean', 'city_price_median']
    
    train = train.merge(city_stats, on='city_code', how='left')
    test = test.merge(city_stats, on='city_code', how='left')
    
    print(f"  - å¸‚åŒºç”ºæ‘åˆ¥çµ±è¨ˆ: {len(city_stats)}ä»¶")
    
    # ç¯‰å¹´æ•°ãƒ»é¢ç©ã®åœ°åŸŸå¹³å‡ã¨ã®æ¯”
    for df in [train, test]:
        # ç¯‰å¹´æ•°ãŒåœ°åŸŸå¹³å‡ã®ä½•å€ã‹
        df['year_built_ratio'] = df['year_built'] / df['city_year_built_mean'].replace(0, np.nan)
        # å°‚æœ‰é¢ç©ãŒåœ°åŸŸå¹³å‡ã®ä½•å€ã‹
        df['unit_area_ratio'] = df['unit_area'] / df['city_unit_area_mean'].replace(0, np.nan)
        # å¹³ç±³å˜ä¾¡ãŒåœ°åŸŸå¹³å‡ã®ä½•å€ã‹ï¼ˆtrainã®ã¿æ„å‘³ãŒã‚ã‚‹ï¼‰
        df['price_sqm_city_ratio'] = df.get('price_per_sqm', 0) / df['city_price_sqm_mean'].replace(0, np.nan)
    
    return train, test


def create_additional_features(train, test):
    """
    è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """
    print("è¿½åŠ ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    for df in [train, test]:
        # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
        
        # ç¯‰å¹´æ•°ã®è¨ˆç®—
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
            df['building_age'] = df['building_age'].clip(lower=0)  # è² ã®å€¤ã‚’0ã«
        
        # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
        if 'total_floor_area' in df.columns:
            df['unit_total_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
        
        # éšæ•°é–¢é€£
        if 'room_floor' in df.columns and 'floor_count' in df.columns:
            df['floor_ratio'] = df['room_floor'] / (df['floor_count'] + 1)
        
        # é§…è·é›¢é–¢é€£
        if 'walk_distance1' in df.columns:
            df['walk_distance1_log'] = np.log1p(df['walk_distance1'].fillna(0))
        if 'walk_distance2' in df.columns:
            df['walk_distance2_log'] = np.log1p(df['walk_distance2'].fillna(0))
        
        # ç®¡ç†è²»ãƒ»ä¿®ç¹•ç©ç«‹é‡‘
        if 'money_kyoueki' in df.columns:
            df['kyoueki_per_sqm'] = df['money_kyoueki'] / (df['unit_area'] + 1)
        if 'money_shuuzen' in df.columns:
            df['shuuzen_per_sqm'] = df['money_shuuzen'] / (df['unit_area'] + 1)
    
    return train, test


def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆå¹³ç±³å˜ä¾¡ï¼‰
    y_train = train['price_per_sqm'].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = ['money_room', 'price_per_sqm', 'id', 'price_sqm_city_ratio']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses', 'city_code']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    new_date_cols = []
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    year_col = f'{col}_year'
                    month_col = f'{col}_month'
                    df[year_col] = df[col].dt.year
                    df[month_col] = df[col].dt.month
                    if year_col not in new_date_cols:
                        new_date_cols.extend([year_col, month_col])
    
    feature_cols.extend(new_date_cols)
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    print(f"  - æ•°å€¤ç‰¹å¾´é‡: {len([c for c in use_cols if c in num_cols])}")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len([c for c in use_cols if c in cat_cols])}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    label_encoders = {}
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            label_encoders[col] = le
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols


def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆMAEä½¿ç”¨ + seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    n_splits = 5
    seeds = [42, 123, 456]  # è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    all_models = []
    oof_pred = np.zeros(len(X_train))
    
    for seed in seeds:
        print(f"\n{'='*40}")
        print(f"Seed: {seed}")
        print(f"{'='*40}")
        
        # MAEã‚’ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
        params = {
            'objective': 'regression',
            'metric': 'mae',
            'boosting_type': 'gbdt',
            'learning_rate': 0.05,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 20,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'n_jobs': -1,
            'seed': seed
        }
        
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        
        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
            print(f"\nFold {fold + 1}/{n_splits}")
            
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train[train_idx], y_train[valid_idx]
            
            cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
            
            train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
            valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=3000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(period=500)
                ]
            )
            
            all_models.append(model)
            
            # OOFäºˆæ¸¬
            if seed == seeds[0]:  # æœ€åˆã®seedã®ã¿OOFè¨ˆç®—
                oof_pred[valid_idx] = model.predict(X_val)
    
    # OOF MAEï¼ˆå¹³ç±³å˜ä¾¡ï¼‰
    oof_mae = np.mean(np.abs(y_train - oof_pred))
    print(f"\n=== OOF MAE (å¹³ç±³å˜ä¾¡): {oof_mae:,.2f}å††/mÂ² ===")
    
    print(f"\nå­¦ç¿’å®Œäº†: åˆè¨ˆ {len(all_models)} ãƒ¢ãƒ‡ãƒ«")
    return all_models


def predict_and_submit(models, X_test, test_unit_area, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§å¹³ç±³å˜ä¾¡ã‚’äºˆæ¸¬
    price_per_sqm_pred = np.zeros(len(X_test))
    for model in models:
        price_per_sqm_pred += model.predict(X_test)
    price_per_sqm_pred /= len(models)
    
    # å¹³ç±³å˜ä¾¡ Ã— å°‚æœ‰é¢ç© = å£²è²·ä¾¡æ ¼
    predictions = price_per_sqm_pred * test_unit_area
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 100000)  # æœ€ä½10ä¸‡å††
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v2.csv'
    submission.to_csv(output_path, index=False, header=False)
    
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"\näºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    print(f"\nå¹³ç±³å˜ä¾¡çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {price_per_sqm_pred.mean():,.0f}å††/mÂ²")
    print(f"  - ä¸­å¤®å€¤: {np.median(price_per_sqm_pred):,.0f}å††/mÂ²")
    
    return submission


def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ v2")
    print("å‚è€ƒ: ç¬¬1å›åœ°ç†ç©ºé–“æƒ…å ±ãƒ‡ãƒ¼ã‚¿ãƒãƒ£ãƒ¬ãƒ³ã‚¸ 1ä½è§£æ³•")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # å¹³ç±³å˜ä¾¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ & åŒä¸€å»ºç‰©æƒ…å ±
    train, test = create_target_features(train, test)
    
    # ä½ç½®æƒ…å ±ç‰¹å¾´é‡
    train, test = create_location_features(train, test)
    
    # è¿½åŠ ç‰¹å¾´é‡
    train, test = create_additional_features(train, test)
    
    # å°‚æœ‰é¢ç©ã‚’ä¿å­˜ï¼ˆæœ€å¾Œã«æ›ã‘ç®—ã™ã‚‹ï¼‰
    test_unit_area = test['unit_area'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_unit_area, test_ids)
    
    print("\n" + "=" * 60)
    print("å‡¦ç†å®Œäº†!")
    print("=" * 60)
    
    return submission


if __name__ == '__main__':
    main()

```

æ”¹å–„ç‰ˆã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¾ã—ãŸã€‚å®Ÿè¡Œã—ã¾ã™ã€‚

---

**User**

ç›®çš„å¤‰æ•°ã‚’å…ƒã®å®¶è³ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã—ã¦ã„ã‚‹éƒ¨åˆ†ã¯ã©ã“ã§ã™ã‹ï¼Ÿ

---

**Cursor**

`predict_and_submit`é–¢æ•°ã®ä¸­ã§è¡Œã£ã¦ã„ã¾ã™ã€‚è©²å½“éƒ¨åˆ†ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ï¼š

è©²å½“éƒ¨åˆ†ã¯ **352-353è¡Œç›®** ã§ã™ï¼š

```352:353:/home/ubuntu/signate/geospatial_data_challenge_2nd/src/predict_v2.py
    # å¹³ç±³å˜ä¾¡ Ã— å°‚æœ‰é¢ç© = å£²è²·ä¾¡æ ¼
    predictions = price_per_sqm_pred * test_unit_area
```

## å‡¦ç†ã®æµã‚Œ

1. **å­¦ç¿’æ™‚**: `price_per_sqm = money_room / unit_area`ï¼ˆå¹³ç±³å˜ä¾¡ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ï¼‰
2. **äºˆæ¸¬æ™‚**: ãƒ¢ãƒ‡ãƒ«ãŒå¹³ç±³å˜ä¾¡ï¼ˆ`price_per_sqm_pred`ï¼‰ã‚’äºˆæ¸¬
3. **å¤‰æ›æ™‚**: `predictions = price_per_sqm_pred * test_unit_area`ï¼ˆå°‚æœ‰é¢ç©ã‚’æ›ã‘ã¦å…ƒã®å£²è²·ä¾¡æ ¼ã«æˆ»ã™ï¼‰

`test_unit_area`ã¯`main()`é–¢æ•°å†…ã§äº‹å‰ã«ä¿å­˜ã—ã¦ã„ã¾ã™ï¼š

```python
# å°‚æœ‰é¢ç©ã‚’ä¿å­˜ï¼ˆæœ€å¾Œã«æ›ã‘ç®—ã™ã‚‹ï¼‰
test_unit_area = test['unit_area'].values
```

ã“ã‚Œã«ã‚ˆã‚Šã€å¹³ç±³å˜ä¾¡ã®äºˆæ¸¬å€¤ã«å„ç‰©ä»¶ã®å°‚æœ‰é¢ç©ã‚’æ›ã‘ç®—ã—ã¦ã€æœ€çµ‚çš„ãªå£²è²·ä¾¡æ ¼ï¼ˆå††ï¼‰ã‚’ç®—å‡ºã—ã¦ã„ã¾ã™ã€‚

```python
    # å¹³ç±³å˜ä¾¡ Ã— å°‚æœ‰é¢ç© = å£²è²·ä¾¡æ ¼
    predictions = price_per_sqm_pred * test_unit_area
```

```python
# å°‚æœ‰é¢ç©ã‚’ä¿å­˜ï¼ˆæœ€å¾Œã«æ›ã‘ç®—ã™ã‚‹ï¼‰
test_unit_area = test['unit_area'].values
```

---

**User**

ã‚¹ã‚³ã‚¢ãŒ42ã«æ‚ªåŒ–ã—ã¾ã—ãŸï¼

---

**Cursor**

ã‚¹ã‚³ã‚¢ãŒæ‚ªåŒ–ã—ã¦ã—ã¾ã„ã¾ã—ãŸã­ã€‚åŸå› ã‚’åˆ†æã—ã¾ã™ã€‚

## å•é¡Œç‚¹ã®åˆ†æ

äºˆæ¸¬çµ±è¨ˆã‚’æ¯”è¼ƒã™ã‚‹ã¨ï¼š

| æŒ‡æ¨™ | v1ï¼ˆå…ƒã®ãƒ¢ãƒ‡ãƒ«ï¼‰ | v2ï¼ˆæ”¹å–„ç‰ˆï¼‰ |
|------|-----------------|-------------|
| å¹³å‡ | 28,365,291å†† | 17,544,438å†† |
| ä¸­å¤®å€¤ | 24,279,791å†† | 9,373,938å†† |
| æœ€å¤§ | 170,559,245å†† | **1,776,668,788å††** |

**å•é¡Œ**: 
- å¹³ç±³å˜ä¾¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§æ¥µç«¯ãªå¤–ã‚Œå€¤ï¼ˆ17å„„å††ï¼‰ãŒç™ºç”Ÿ
- å…¨ä½“çš„ã«äºˆæ¸¬å€¤ãŒä½ããªã‚Šã™ãã¦ã„ã‚‹

## ä¿®æ­£æ–¹é‡

å¹³ç±³å˜ä¾¡ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯è³ƒæ–™äºˆæ¸¬ã§ã¯æœ‰åŠ¹ã§ã—ãŸãŒã€å£²è²·ä¾¡æ ¼ã§ã¯ç•°ãªã‚‹ç‰¹æ€§ãŒã‚ã‚‹ã‚ˆã†ã§ã™ã€‚**å…ƒã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆmoney_roomã‚’ç›´æ¥äºˆæ¸¬ï¼‰ã«æˆ»ã—ã¤ã¤**ã€è¨˜äº‹ã‹ã‚‰æœ‰åŠ¹ãªæ”¹å–„ç‚¹ã ã‘ã‚’å–ã‚Šå…¥ã‚Œã¾ã™ï¼š

1. åŒä¸€å»ºç‰©(building_id)ã®ä¾¡æ ¼çµ±è¨ˆ
2. éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã®ç›¸å ´æƒ…å ±
3. è¤‡æ•°seedã§ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ v3
å…ƒã®money_roomç›´æ¥äºˆæ¸¬ã«æˆ»ã—ã¤ã¤ã€æœ‰åŠ¹ãªç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’é©ç”¨

æ”¹å–„ç‚¹:
1. åŒä¸€å»ºç‰©(building_id)ã®ä¾¡æ ¼çµ±è¨ˆã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
2. éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã«è¿½åŠ 
3. è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
4. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯money_roomã‚’ç›´æ¥äºˆæ¸¬ï¼ˆå¹³ç±³å˜ä¾¡ã§ã¯ãªãï¼‰
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'


def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test


def mape(y_true, y_pred):
    """MAPEè¨ˆç®—"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


def mape_lgb(y_pred, data):
    """LightGBMç”¨MAPEãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    y_true = data.get_label()
    score = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return 'mape', score, False


def create_building_features(train, test):
    """
    åŒä¸€å»ºç‰©(building_id)ã®ä¾¡æ ¼æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    """
    print("å»ºç‰©æƒ…å ±ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # åŒä¸€å»ºç‰©(building_id)ã®ä¾¡æ ¼çµ±è¨ˆã‚’trainã‹ã‚‰è¨ˆç®—
    building_stats = train.groupby('building_id').agg({
        'money_room': ['mean', 'std', 'min', 'max', 'median', 'count']
    }).reset_index()
    building_stats.columns = ['building_id', 'bld_price_mean', 'bld_price_std', 
                               'bld_price_min', 'bld_price_max', 'bld_price_median', 'bld_count']
    
    # stdãŒNaNã®å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    building_stats['bld_price_std'] = building_stats['bld_price_std'].fillna(0)
    
    train = train.merge(building_stats, on='building_id', how='left')
    test = test.merge(building_stats, on='building_id', how='left')
    
    print(f"  - åŒä¸€å»ºç‰©æƒ…å ±: {len(building_stats)}ä»¶ã®building_id")
    
    # å°‚æœ‰é¢ç©ã®ä¸­å¤®å€¤ã‚’è¨ˆç®—
    unit_area_median = train['unit_area'].median()
    
    # å¹³ç±³å˜ä¾¡ã‚‚è¨ˆç®—ã—ã¦ç‰¹å¾´é‡ã«ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã¯ãªãç‰¹å¾´é‡ã¨ã—ã¦ï¼‰
    for df in [train, test]:
        df['unit_area_filled'] = df['unit_area'].fillna(unit_area_median)
        df['unit_area_filled'] = df['unit_area_filled'].replace(0, unit_area_median)
    
    # å»ºç‰©ã”ã¨ã®å¹³ç±³å˜ä¾¡çµ±è¨ˆ
    train['price_per_sqm_temp'] = train['money_room'] / train['unit_area_filled']
    building_sqm_stats = train.groupby('building_id').agg({
        'price_per_sqm_temp': ['mean', 'std', 'median']
    }).reset_index()
    building_sqm_stats.columns = ['building_id', 'bld_sqm_price_mean', 'bld_sqm_price_std', 'bld_sqm_price_median']
    building_sqm_stats['bld_sqm_price_std'] = building_sqm_stats['bld_sqm_price_std'].fillna(0)
    
    train = train.merge(building_sqm_stats, on='building_id', how='left')
    test = test.merge(building_sqm_stats, on='building_id', how='left')
    
    # ä¸€æ™‚ã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    train = train.drop(columns=['price_per_sqm_temp'])
    
    return train, test


def create_location_features(train, test):
    """
    éƒµä¾¿ç•ªå·ãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã®ç›¸å ´æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦è¿½åŠ 
    """
    print("ä½ç½®æƒ…å ±ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    # éƒµä¾¿ç•ªå·(post1)ã”ã¨ã®ä¾¡æ ¼çµ±è¨ˆ
    post_stats = train.groupby('post1').agg({
        'money_room': ['mean', 'std', 'median', 'count']
    }).reset_index()
    post_stats.columns = ['post1', 'post_price_mean', 'post_price_std', 
                          'post_price_median', 'post_count']
    post_stats['post_price_std'] = post_stats['post_price_std'].fillna(0)
    
    train = train.merge(post_stats, on='post1', how='left')
    test = test.merge(post_stats, on='post1', how='left')
    
    print(f"  - éƒµä¾¿ç•ªå·åˆ¥çµ±è¨ˆ: {len(post_stats)}ä»¶")
    
    # å¸‚åŒºç”ºæ‘(addr1_1 + addr1_2)ã”ã¨ã®çµ±è¨ˆ
    for df in [train, test]:
        df['city_code'] = df['addr1_1'].astype(str) + '_' + df['addr1_2'].astype(str)
    
    city_stats = train.groupby('city_code').agg({
        'money_room': ['mean', 'std', 'median'],
        'year_built': ['mean'],
        'unit_area': ['mean']
    }).reset_index()
    city_stats.columns = ['city_code', 'city_price_mean', 'city_price_std',
                          'city_price_median', 'city_year_built_mean', 
                          'city_unit_area_mean']
    city_stats['city_price_std'] = city_stats['city_price_std'].fillna(0)
    
    train = train.merge(city_stats, on='city_code', how='left')
    test = test.merge(city_stats, on='city_code', how='left')
    
    print(f"  - å¸‚åŒºç”ºæ‘åˆ¥çµ±è¨ˆ: {len(city_stats)}ä»¶")
    
    # ç¯‰å¹´æ•°ãƒ»é¢ç©ã®åœ°åŸŸå¹³å‡ã¨ã®æ¯”
    for df in [train, test]:
        df['year_built_ratio'] = df['year_built'] / df['city_year_built_mean'].replace(0, np.nan)
        df['unit_area_ratio'] = df['unit_area'] / df['city_unit_area_mean'].replace(0, np.nan)
    
    return train, test


def create_additional_features(train, test):
    """
    è¿½åŠ ã®ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    """
    print("è¿½åŠ ç‰¹å¾´é‡ä½œæˆä¸­...")
    
    for df in [train, test]:
        # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
        
        # ç¯‰å¹´æ•°ã®è¨ˆç®—
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
            df['building_age'] = df['building_age'].clip(lower=0)
        
        # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
        if 'total_floor_area' in df.columns:
            df['unit_total_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
        
        # éšæ•°é–¢é€£
        if 'room_floor' in df.columns and 'floor_count' in df.columns:
            df['floor_ratio'] = df['room_floor'] / (df['floor_count'] + 1)
        
        # é§…è·é›¢é–¢é€£
        if 'walk_distance1' in df.columns:
            df['walk_distance1_log'] = np.log1p(df['walk_distance1'].fillna(0))
        if 'walk_distance2' in df.columns:
            df['walk_distance2_log'] = np.log1p(df['walk_distance2'].fillna(0))
        
        # ç®¡ç†è²»ãƒ»ä¿®ç¹•ç©ç«‹é‡‘ï¼ˆé¢ç©ã‚ãŸã‚Šï¼‰
        if 'money_kyoueki' in df.columns and 'unit_area_filled' in df.columns:
            df['kyoueki_per_sqm'] = df['money_kyoueki'] / (df['unit_area_filled'] + 1)
        if 'money_shuuzen' in df.columns and 'unit_area_filled' in df.columns:
            df['shuuzen_per_sqm'] = df['money_shuuzen'] / (df['unit_area_filled'] + 1)
    
    return train, test


def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ï¼ˆmoney_roomã‚’ç›´æ¥äºˆæ¸¬ï¼‰
    target = 'money_room'
    y_train = train[target].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = [target, 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses', 'city_code']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    new_date_cols = []
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    year_col = f'{col}_year'
                    month_col = f'{col}_month'
                    df[year_col] = df[col].dt.year
                    df[month_col] = df[col].dt.month
                    if year_col not in new_date_cols:
                        new_date_cols.extend([year_col, month_col])
    
    feature_cols.extend(new_date_cols)
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    print(f"  - æ•°å€¤ç‰¹å¾´é‡: {len([c for c in use_cols if c in num_cols])}")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len([c for c in use_cols if c in cat_cols])}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols


def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆMAPEæœ€é©åŒ– + seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    n_splits = 5
    seeds = [42, 123, 456]  # è¤‡æ•°seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    all_models = []
    oof_pred = np.zeros(len(X_train))
    
    for seed in seeds:
        print(f"\n{'='*40}")
        print(f"Seed: {seed}")
        print(f"{'='*40}")
        
        params = {
            'objective': 'regression',
            'metric': 'mape',
            'boosting_type': 'gbdt',
            'learning_rate': 0.05,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 20,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'n_jobs': -1,
            'seed': seed
        }
        
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        
        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
            print(f"\nFold {fold + 1}/{n_splits}")
            
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train[train_idx], y_train[valid_idx]
            
            cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
            
            train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
            valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=3000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                feval=mape_lgb,
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(period=500)
                ]
            )
            
            all_models.append(model)
            
            # OOFäºˆæ¸¬ï¼ˆæœ€åˆã®seedã®ã¿ï¼‰
            if seed == seeds[0]:
                oof_pred[valid_idx] = model.predict(X_val)
    
    # OOF MAPE
    oof_mape = mape(y_train, oof_pred)
    print(f"\n=== OOF MAPE: {oof_mape:.4f}% ===")
    
    print(f"\nå­¦ç¿’å®Œäº†: åˆè¨ˆ {len(all_models)} ãƒ¢ãƒ‡ãƒ«")
    return all_models


def predict_and_submit(models, X_test, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§äºˆæ¸¬
    predictions = np.zeros(len(X_test))
    for model in models:
        predictions += model.predict(X_test)
    predictions /= len(models)
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v3.csv'
    submission.to_csv(output_path, index=False, header=False)
    
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"\näºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    
    return submission


def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - æ”¹å–„ç‰ˆ v3")
    print("money_roomã‚’ç›´æ¥äºˆæ¸¬ + ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å¼·åŒ–")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # å»ºç‰©æƒ…å ±ç‰¹å¾´é‡
    train, test = create_building_features(train, test)
    
    # ä½ç½®æƒ…å ±ç‰¹å¾´é‡
    train, test = create_location_features(train, test)
    
    # è¿½åŠ ç‰¹å¾´é‡
    train, test = create_additional_features(train, test)
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_ids)
    
    print("\n" + "=" * 60)
    print("å‡¦ç†å®Œäº†!")
    print("=" * 60)
    
    return submission


if __name__ == '__main__':
    main()

```

---

**User**

ã‚¹ã‚³ã‚¢ãŒ40ã§ã—ãŸï¼å…¨ç„¶æ”¹å–„ã—ã¾ã›ã‚“ã­

---

**Cursor**

äºˆæ¸¬å€¤ã®çµ±è¨ˆã‚’æ¯”è¼ƒã™ã‚‹ã¨æ˜ã‚‰ã‹ãªå•é¡ŒãŒã‚ã‚Šã¾ã™ï¼š

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | å¹³å‡ | ä¸­å¤®å€¤ | ã‚¹ã‚³ã‚¢ |
|-----------|------|--------|--------|
| v1ï¼ˆå…ƒï¼‰ | 28,365,291å†† | 24,279,791å†† | **17** |
| v3ï¼ˆç¾åœ¨ï¼‰ | 17,692,455å†† | 9,448,123å†† | **40** |

**å•é¡Œ**: è¿½åŠ ã—ãŸç‰¹å¾´é‡ãŒäºˆæ¸¬ã‚’ä½ãæ­ªã‚ã¦ã„ã¾ã™ã€‚å»ºç‰©ãƒ»éƒµä¾¿ç•ªå·ã®çµ±è¨ˆãŒtestãƒ‡ãƒ¼ã‚¿ã§æ¬ æâ†’-999ã§åŸ‹ã‚ã‚‰ã‚Œæ‚ªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ã€‚

å…ƒã®v1ã«æˆ»ã—ã¦ã€æœ€å°é™ã®æ”¹å–„ã ã‘ã‚’è©¦ã¿ã¾ã—ã‚‡ã†ï¼š

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - v4
v1ãƒ™ãƒ¼ã‚¹ã§æœ€å°é™ã®æ”¹å–„ã®ã¿é©ç”¨
- è¤‡æ•°seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
- å¤–éƒ¨çµ±è¨ˆç‰¹å¾´é‡ã¯ä½¿ç”¨ã—ãªã„ï¼ˆãƒªãƒ¼ã‚¯å›é¿ï¼‰
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'


def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test


def mape(y_true, y_pred):
    """MAPEè¨ˆç®—"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


def mape_lgb(y_pred, data):
    """LightGBMç”¨MAPEãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    y_true = data.get_label()
    score = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    return 'mape', score, False


def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†ï¼ˆv1ãƒ™ãƒ¼ã‚¹ï¼‰"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = 'money_room'
    y_train = train[target].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = [target, 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    # é‡è¤‡å‰Šé™¤
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
    for df in [train, test]:
        if 'unit_area' in df.columns and 'total_floor_area' in df.columns:
            df['unit_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
    
    for col in ['unit_area_ratio', 'land_coverage', 'house_land_ratio']:
        if col in train.columns and col not in num_cols:
            num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    print(f"  - æ•°å€¤ç‰¹å¾´é‡: {len([c for c in use_cols if c in num_cols])}")
    print(f"  - ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡: {len([c for c in use_cols if c in cat_cols])}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    label_encoders = {}
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            label_encoders[col] = le
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols


def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆè¤‡æ•°seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    n_splits = 5
    seeds = [42, 123, 456, 789, 2024]  # 5ã¤ã®seedã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
    
    all_models = []
    oof_pred_sum = np.zeros(len(X_train))
    oof_count = np.zeros(len(X_train))
    
    for seed in seeds:
        print(f"\n{'='*40}")
        print(f"Seed: {seed}")
        print(f"{'='*40}")
        
        params = {
            'objective': 'regression',
            'metric': 'mape',
            'boosting_type': 'gbdt',
            'learning_rate': 0.05,
            'num_leaves': 127,
            'max_depth': -1,
            'min_child_samples': 20,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'n_jobs': -1,
            'seed': seed
        }
        
        kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)
        
        for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
            print(f"Fold {fold + 1}/{n_splits}", end=" ")
            
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
            y_tr, y_val = y_train[train_idx], y_train[valid_idx]
            
            cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
            
            train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
            valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
            
            model = lgb.train(
                params,
                train_data,
                num_boost_round=2000,
                valid_sets=[train_data, valid_data],
                valid_names=['train', 'valid'],
                feval=mape_lgb,
                callbacks=[
                    lgb.early_stopping(stopping_rounds=100, verbose=False),
                    lgb.log_evaluation(period=0)  # ãƒ­ã‚°éè¡¨ç¤º
                ]
            )
            
            all_models.append(model)
            
            # OOFäºˆæ¸¬
            pred = model.predict(X_val)
            oof_pred_sum[valid_idx] += pred
            oof_count[valid_idx] += 1
            
            fold_mape = mape(y_val, pred)
            print(f"MAPE: {fold_mape:.4f}%")
    
    # OOF MAPEï¼ˆå¹³å‡ï¼‰
    oof_pred = oof_pred_sum / oof_count
    oof_mape = mape(y_train, oof_pred)
    print(f"\n=== Overall OOF MAPE: {oof_mape:.4f}% ===")
    
    print(f"\nå­¦ç¿’å®Œäº†: åˆè¨ˆ {len(all_models)} ãƒ¢ãƒ‡ãƒ«")
    return all_models


def predict_and_submit(models, X_test, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§äºˆæ¸¬
    predictions = np.zeros(len(X_test))
    for model in models:
        predictions += model.predict(X_test)
    predictions /= len(models)
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v4.csv'
    submission.to_csv(output_path, index=False, header=False)
    
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"\näºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    
    return submission


def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - v4")
    print("v1ãƒ™ãƒ¼ã‚¹ + è¤‡æ•°seedã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_ids)
    
    print("\n" + "=" * 60)
    print("å‡¦ç†å®Œäº†!")
    print("=" * 60)
    
    return submission


if __name__ == '__main__':
    main()

```

---

**User**

17.4ã«ä¸ŠãŒã‚Šã¾ã—ãŸ

---

**Cursor**

v4ã§ã‚¹ã‚³ã‚¢17.4ã§ã™ã­ã€‚v1ã®17ã¨æ¯”è¼ƒã™ã‚‹ã¨å°‘ã—æ‚ªåŒ–ã—ã¦ã„ã¾ã™ï¼ˆMAPEã¯å°ã•ã„ã»ã©è‰¯ã„ï¼‰ã€‚ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãŒé€†åŠ¹æœã ã£ãŸã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

æ¬¡ã¯**å¯¾æ•°å¤‰æ›**ã‚’è©¦ã—ã¾ã—ã‚‡ã†ã€‚ä¸å‹•ç”£ä¾¡æ ¼ã¯å¯¾æ•°æ­£è¦åˆ†å¸ƒã«è¿‘ã„ã“ã¨ãŒå¤šãã€å¯¾æ•°å¤‰æ›ã™ã‚‹ã“ã¨ã§äºˆæ¸¬ç²¾åº¦ãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```python
"""
ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - v5
v1ãƒ™ãƒ¼ã‚¹ + å¯¾æ•°å¤‰æ›
- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ(money_room)ã‚’å¯¾æ•°å¤‰æ›ã—ã¦å­¦ç¿’
- äºˆæ¸¬å¾Œã«exp()ã§å…ƒã«æˆ»ã™
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import LabelEncoder
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/raw'
OUTPUT_DIR = '/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit'


def load_data():
    """ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
    train = pd.read_csv(f'{DATA_DIR}/train.csv')
    test = pd.read_csv(f'{DATA_DIR}/test.csv')
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    return train, test


def mape(y_true, y_pred):
    """MAPEè¨ˆç®—"""
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100


def preprocess_features(train, test):
    """ç‰¹å¾´é‡ã®å‰å‡¦ç†ï¼ˆv1ãƒ™ãƒ¼ã‚¹ï¼‰"""
    print("ç‰¹å¾´é‡å‰å‡¦ç†ä¸­...")
    
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°
    target = 'money_room'
    y_train = train[target].values
    
    # ä¸è¦ãªã‚«ãƒ©ãƒ ã‚’å‰Šé™¤
    drop_cols = [target, 'id']
    
    # æ—¥ä»˜é–¢é€£ã®ã‚«ãƒ©ãƒ ã‚’ç‰¹å®š
    date_cols = [col for col in train.columns if 'date' in col.lower()]
    
    # ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚«ãƒ©ãƒ 
    text_cols = ['building_name', 'building_name_ruby', 'homes_building_name', 
                 'homes_building_name_ruby', 'full_address', 'unit_name', 'name_ruby',
                 'addr2_name', 'addr3_name', 'rosen_name1', 'eki_name1', 'bus_stop1',
                 'rosen_name2', 'eki_name2', 'bus_stop2', 'traffic_other', 'traffic_car',
                 'school_ele_name', 'school_jun_name', 'reform_exterior_other',
                 'reform_common_area', 'reform_place', 'reform_place_other',
                 'reform_wet_area', 'reform_wet_area_other', 'reform_interior',
                 'reform_interior_other', 'reform_etc', 'renovation_etc',
                 'money_sonota_str1', 'money_sonota_str2', 'money_sonota_str3',
                 'parking_memo', 'empty_contents', 'land_seigen', 'est_other_name',
                 'building_tag_id', 'unit_tag_id', 'statuses']
    
    # ä½¿ç”¨ã™ã‚‹ã‚«ãƒ©ãƒ 
    feature_cols = [col for col in train.columns 
                    if col not in drop_cols + date_cols + text_cols]
    
    # æ—¥ä»˜ã‹ã‚‰ã®ç‰¹å¾´é‡æŠ½å‡º
    for df in [train, test]:
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                if df[col].notna().any():
                    df[f'{col}_year'] = df[col].dt.year
                    df[f'{col}_month'] = df[col].dt.month
                    feature_cols.extend([f'{col}_year', f'{col}_month'])
    
    # é‡è¤‡å‰Šé™¤
    feature_cols = list(set(feature_cols))
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ»æ•°å€¤ã‚«ãƒ©ãƒ ã®åˆ†é¡
    cat_cols = []
    num_cols = []
    
    for col in feature_cols:
        if col in train.columns and col in test.columns:
            if train[col].dtype == 'object':
                cat_cols.append(col)
            else:
                num_cols.append(col)
    
    # target_ymã‹ã‚‰å¹´æœˆã‚’æŠ½å‡º
    for df in [train, test]:
        df['year'] = df['target_ym'] // 100
        df['month'] = df['target_ym'] % 100
    
    if 'year' not in num_cols:
        num_cols.extend(['year', 'month'])
    
    # ç¯‰å¹´æ•°ã®è¨ˆç®—
    for df in [train, test]:
        if 'year_built' in df.columns:
            df['building_age'] = df['year'] - (df['year_built'] // 100)
    
    if 'building_age' not in num_cols:
        num_cols.append('building_age')
    
    # é¢ç©é–¢é€£ã®ç‰¹å¾´é‡
    for df in [train, test]:
        if 'unit_area' in df.columns and 'total_floor_area' in df.columns:
            df['unit_area_ratio'] = df['unit_area'] / (df['total_floor_area'] + 1)
        if 'building_land_area' in df.columns and 'land_area_all' in df.columns:
            df['land_coverage'] = df['building_land_area'] / (df['land_area_all'] + 1)
        if 'house_area' in df.columns and 'snapshot_land_area' in df.columns:
            df['house_land_ratio'] = df['house_area'] / (df['snapshot_land_area'] + 1)
    
    for col in ['unit_area_ratio', 'land_coverage', 'house_land_ratio']:
        if col in train.columns and col not in num_cols:
            num_cols.append(col)
    
    # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ã‚’æœ€çµ‚æ±ºå®š
    use_cols = num_cols + cat_cols
    use_cols = [col for col in use_cols if col in train.columns and col in test.columns]
    
    print(f"ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡æ•°: {len(use_cols)}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    X_train = train[use_cols].copy()
    X_test = test[use_cols].copy()
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
    for col in cat_cols:
        if col in X_train.columns:
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
    
    # æ¬ æå€¤å‡¦ç†
    X_train = X_train.fillna(-999)
    X_test = X_test.fillna(-999)
    
    # ã™ã¹ã¦ã®ã‚«ãƒ©ãƒ ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in X_train.columns:
        if X_train[col].dtype == 'object':
            le = LabelEncoder()
            combined = pd.concat([X_train[col].astype(str), X_test[col].astype(str)])
            le.fit(combined)
            X_train[col] = le.transform(X_train[col].astype(str))
            X_test[col] = le.transform(X_test[col].astype(str))
            if col not in cat_cols:
                cat_cols.append(col)
        else:
            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(-999)
            X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(-999)
    
    return X_train, X_test, y_train, use_cols, cat_cols


def train_model(X_train, y_train, cat_cols):
    """LightGBMãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆå¯¾æ•°å¤‰æ›ç‰ˆï¼‰"""
    print("\nãƒ¢ãƒ‡ãƒ«å­¦ç¿’ä¸­...")
    
    # å¯¾æ•°å¤‰æ›
    y_train_log = np.log1p(y_train)
    
    n_splits = 5
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    
    models = []
    oof_pred_log = np.zeros(len(X_train))
    
    params = {
        'objective': 'regression',
        'metric': 'rmse',  # å¯¾æ•°ç©ºé–“ã§ã¯RMSEã‚’ä½¿ç”¨
        'boosting_type': 'gbdt',
        'learning_rate': 0.05,
        'num_leaves': 127,
        'max_depth': -1,
        'min_child_samples': 20,
        'feature_fraction': 0.8,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'verbose': -1,
        'n_jobs': -1,
        'seed': 42
    }
    
    for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train)):
        print(f"\nFold {fold + 1}/{n_splits}")
        
        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]
        y_tr, y_val = y_train_log[train_idx], y_train_log[valid_idx]
        
        cat_indices = [X_train.columns.get_loc(col) for col in cat_cols if col in X_train.columns]
        
        train_data = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_indices)
        valid_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_indices)
        
        model = lgb.train(
            params,
            train_data,
            num_boost_round=2000,
            valid_sets=[train_data, valid_data],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(stopping_rounds=100, verbose=True),
                lgb.log_evaluation(period=200)
            ]
        )
        
        models.append(model)
        oof_pred_log[valid_idx] = model.predict(X_val)
        
        # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã§MAPEè¨ˆç®—
        y_val_orig = y_train[valid_idx]
        pred_orig = np.expm1(oof_pred_log[valid_idx])
        fold_mape = mape(y_val_orig, pred_orig)
        print(f"Fold {fold + 1} MAPE: {fold_mape:.4f}%")
    
    # OOF MAPEï¼ˆå…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    oof_pred = np.expm1(oof_pred_log)
    oof_mape = mape(y_train, oof_pred)
    print(f"\n=== Overall OOF MAPE: {oof_mape:.4f}% ===")
    
    return models


def predict_and_submit(models, X_test, test_ids):
    """äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    print("\näºˆæ¸¬ä¸­...")
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®å¹³å‡ã§äºˆæ¸¬ï¼ˆå¯¾æ•°ç©ºé–“ï¼‰
    predictions_log = np.zeros(len(X_test))
    for model in models:
        predictions_log += model.predict(X_test)
    predictions_log /= len(models)
    
    # å…ƒã®ã‚¹ã‚±ãƒ¼ãƒ«ã«æˆ»ã™
    predictions = np.expm1(predictions_log)
    
    # è² ã®å€¤ã‚’è£œæ­£
    predictions = np.maximum(predictions, 1)
    
    # æ•´æ•°ã«ä¸¸ã‚ã‚‹
    predictions = np.round(predictions).astype(int)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'id': test_ids,
        'money_room': predictions
    })
    submission = submission.sort_values('id')
    
    output_path = f'{OUTPUT_DIR}/submission_v5.csv'
    submission.to_csv(output_path, index=False, header=False)
    
    print(f"\næå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
    print(f"äºˆæ¸¬ä»¶æ•°: {len(submission)}")
    print(f"\näºˆæ¸¬ä¾¡æ ¼çµ±è¨ˆ:")
    print(f"  - å¹³å‡: {predictions.mean():,.0f}å††")
    print(f"  - ä¸­å¤®å€¤: {np.median(predictions):,.0f}å††")
    print(f"  - æœ€å°: {predictions.min():,.0f}å††")
    print(f"  - æœ€å¤§: {predictions.max():,.0f}å††")
    
    return submission


def main():
    print("=" * 60)
    print("ä¸å‹•ç”£ä¾¡æ ¼äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« - v5")
    print("v1ãƒ™ãƒ¼ã‚¹ + å¯¾æ•°å¤‰æ›")
    print("=" * 60)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    train, test = load_data()
    
    # ãƒ†ã‚¹ãƒˆIDã‚’ä¿å­˜
    test_ids = test['id'].values
    
    # ç‰¹å¾´é‡å‰å‡¦ç†
    X_train, X_test, y_train, use_cols, cat_cols = preprocess_features(train, test)
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    models = train_model(X_train, y_train, cat_cols)
    
    # äºˆæ¸¬ãƒ»æå‡º
    submission = predict_and_submit(models, X_test, test_ids)
    
    print("\n" + "=" * 60)
    print("å‡¦ç†å®Œäº†!")
    print("=" * 60)
    
    return submission


if __name__ == '__main__':
    main()

```

ç´ æ™´ã‚‰ã—ã„çµæœã§ã™ï¼

## æ¯”è¼ƒ

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | OOF MAPE | æå‡ºã‚¹ã‚³ã‚¢ |
|-----------|----------|----------|
| v1 | 15.76% | 17 |
| v5ï¼ˆå¯¾æ•°å¤‰æ›ï¼‰ | **14.35%** | ??? |

OOF MAPEãŒ **15.76% â†’ 14.35%** ã«å¤§å¹…æ”¹å–„ã—ã¾ã—ãŸï¼

æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: `/home/ubuntu/signate/geospatial_data_challenge_2nd/data/submit/submission_v5.csv`

äºˆæ¸¬çµ±è¨ˆã‚‚é©åˆ‡ãªç¯„å›²ã§ã™ï¼š
- å¹³å‡: 27,526,140å††
- ä¸­å¤®å€¤: 23,516,101å††

ãœã²æå‡ºã—ã¦ã‚¹ã‚³ã‚¢ã‚’ç¢ºèªã—ã¦ã¿ã¦ãã ã•ã„ï¼

